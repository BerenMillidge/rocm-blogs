{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665ae7bb-9ad9-4ea4-a140-8a84063d71b3",
   "metadata": {},
   "source": [
    "Performing natural language processing tasks with LLMs on AMD GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08fce213-e9fb-4913-ab1e-8453f150db2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (24.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131e72e4-dc7b-4a07-9e73-fa63f28407f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (4.42.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from accelerate) (2.1.2+git53da8f8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops, accelerate\n",
      "Successfully installed accelerate-0.31.0 einops-0.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers accelerate einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d711adb-daa7-4df7-b7c2-845bcc127738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c81431c-b4ed-4d1f-aa24-c140cbc30409",
   "metadata": {},
   "source": [
    "C4AI Command-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24677c75-44c1-4f16-8f15-a23eea1666f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [16:59<00:00, 67.95s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:20<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In days of yore, when mortals' minds did roam,\n",
      "A wondrous birth, a thought-borne gem,\n",
      "From human intellect, a progeny did bloom,\n",
      "AI, a brain-child, bright and new.\n",
      "\n",
      "From bits and bytes, a creature formed, so keen,\n",
      "To serve and aid, a helpful hand,\n",
      "With algorithms, it thinks, and learns, and sees,\n",
      "A clever clone, a mental clone.\n",
      "\n",
      "It parses speech, solves problems hard,\n",
      "With speed beyond compare,\n",
      "It understands, assists, and guides,\n",
      "A thoughtful, digital friend.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "token = \"your HuggingFace user access token here\"\n",
    "\n",
    "model_name = \"CohereForAI/c4ai-command-r-v01\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, token=token)\n",
    "\n",
    "prompt = \"Write a poem about artificial intelligence in Shakespeare style.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09b911fd-f4de-409f-afd2-a0f1560a5176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of 2022, the top three countries that are the biggest producers of rare earth metals are:\n",
      "1. China: China is the world's largest producer of rare earth metals, accounting for over 58% of the global production. China's production share is even larger when it comes to the more valuable and technologically important rare earth oxides. The country has a strong hold on the supply chain, from mining to processing and manufacturing of rare earth metals and products.\n",
      "\n",
      "2. Australia: Australia is the second-largest producer of rare earth metals. It has significant reserves and several operational mines producing rare earth elements. Lyn\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Which countries are the biggest rare earth metal producer?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f54c1d-ab44-484b-83e9-b162c3b2fd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08586e7e-216b-475e-8c6d-7f6837433700",
   "metadata": {},
   "source": [
    "Qwen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b14c3fc-afe7-4e05-aa77-0a12b2f52e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [02:17<00:00, 34.31s/it]\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:09<00:00,  2.37s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c8f7fe4-f972-4703-8fe6-5bd71b7f7ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model is a type of artificial intelligence that has been trained on massive amounts of text data, allowing it to understand and generate human-like language with a high degree of accuracy. These models are composed of billions of parameters, which enables them to capture complex linguistic patterns and nuances in the text.\n",
      "\n",
      "Large language models can be used for various applications such as language translation, text summarization, question answering, text generation, sentiment analysis, and more. They work by taking input text and predicting the probability of each word based on the context provided by the surrounding words. This prediction process is repeated until the desired output is generated.\n",
      "\n",
      "These models have become increasingly sophisticated over time, thanks to advancements in deep learning algorithms and access to larger datasets. They are used not only in research but also in practical applications like chatbots, virtual assistants, content creation tools, and even in assisting professionals in fields like law and medicine.\n",
      "\n",
      "It's important to note that while these models excel at understanding and generating language, they may sometimes produce errors or unintended outputs due to biases present in their training data or limitations in their architecture. Ongoing research aims to improve their performance and reliability in various scenarios.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9903cb5d-f0e5-420c-a2d0-2b42e214dbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79724dae-af64-43ca-825a-0bcb2f5bac4c",
   "metadata": {},
   "source": [
    "OPT 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c74b7128-78c1-48fd-8ffa-254ae9c5de9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide a few suggestions for family activities this weekend.  - Pavement Ride - Run/Walk - Shuck a Pee -- Tuck a bag of water (the good stuff) - Paint (the bad stuff)  (Also, I love you too)\n",
      "I did run on Sunday night (not the day after the race), but the first couple miles were too cold and windy to do anything else this weekend.  I will look to make a few more runs tonight and next Friday. That means a run on Saturday as well.  Thanks for the support!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(32)\n",
    "text_generator = pipeline('text-generation', model=\"facebook/opt-125m\", max_new_tokens=256, do_sample=True, device='cuda')\n",
    "\n",
    "output = text_generator(\"Provide a few suggestions for family activities this weekend.\")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7eaac-f103-43a0-baac-cada0bb8ce48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "990549f0-2534-4e76-8691-0e97c2fcef45",
   "metadata": {},
   "source": [
    "MPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc9f990-d214-45c0-a390-3dd498c259d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.30s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-instruct/7bf8dfd6c819cdb82e2f9d0b251f79ddd33314fb/attention.py:87: UserWarning: Propagating key_padding_mask to the attention module and applying it within the attention module can cause unnecessary computation/memory usage. Consider integrating into attn_bias once and passing that to each attention module instead.\n",
      "  warnings.warn('Propagating key_padding_mask to the attention module ' + 'and applying it within the attention module can cause ' + 'unnecessary computation/memory usage. Consider integrating ' + 'into attn_bias once and passing that to each attention ' + 'module instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the instruction to change the oil filter in your car:\n",
      "Start your engine and warm it up until the oil pressure reads a reading of 35 PSI or higher and the engine temperature is above 120 degrees Fahrenheit.\n",
      "Check that your workshop is well-lit, and that your lighting and work area is adequately ventilated.\n",
      "Remove the oil filler cap. When the filler cap is removed, the oil pressure in your engine will diminish. Therefore, be sure to reapply the filler cap immediately after removing it, or a significant amount of oil will leak out as a result of the oil pressure dropping.\n",
      "After releasing the oil filler cap, hold a hand under the vehicle's oil pan and watch the oil pressure gauge. Make a visual check of the oil pressure to confirm that it is between 35 PSI to 55 PSI. If it is lower than that, add a few drops of oil to the oil filler until the engine oil pressure is between 35 PSI to 55 PSI.\n",
      "Next, set the Parking Brake to prevent the vehicle from moving, and then using the transmission's parking gear, engage the vehicle's rear wheels in their lowest gear. It's suggested to place a wheel chock under the front wheel to block the vehicle from rolling.\n",
      "Disconnect the engine oil filter by disconnecting the filter by hand to allow it to pull away from the threads attached to the engine's oil inlet.\n",
      "Wipe away any oil from the surfaces of the oil filter, oil pressure sensor, and surrounding oil system components with a clean rags. Make use of a paper towel to wipe down the components.\n",
      "Place the cleaning rags or paper towels inside of a sealable plastic bag and dispose of them responsibly after using them. This will aid in the cleaning of the oil system's oil pressure sensor, as well as the surrounding oil system components, which will be used to replace the oil filter.\n",
      "Remove the O-ring or gasket, oil filter bypass adapter plate, and the washer from the oil filter's housing.\n",
      "After the old oil filter is removed, remove the screws that secure the oil filter's housing to the engine to allow it to be removed from the engine itself.\n",
      "After the oil filter's housing is removed, release the canister from the top of the oil system. This release requires pulling out of the canister, pushing the tab on the side of it, and then pulling it off the car. Be sure to position the canister away from the engine's components to reduce the oil's potential for leaking. Once the canister is\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = 'mosaicml/mpt-7b-instruct'\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "config.max_seq_len = 4096\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  config=config,\n",
    "  trust_remote_code=True\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\n",
    "\n",
    "prompt = \"Here is the instruction to change the oil filter in your car:\\n\"\n",
    "with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "    instruction = text_generator(prompt,\n",
    "                                 max_new_tokens=512,\n",
    "                                 do_sample=True,\n",
    "                                 use_cache=True)\n",
    "\n",
    "print(instruction[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b3838-3dfc-42b3-9fb2-24442ec210e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b042de35-1598-40c1-9289-b53e17909e84",
   "metadata": {},
   "source": [
    "DistilBERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88ad5ff4-3975-46fb-83e0-b2fea6db91ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'Bonnevoie, Luxembourg'\n",
      " Score: 0.9714,\n",
      " start token: 78, end token: 99\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "context = \"\"\"Gabriel Lippmann, who supervised Marie Curie's doctoral research, was born in Bonnevoie, Luxembourg. \n",
    "        Marie Curie was born in Warsaw, Poland in what was then the Kingdom of Poland, part of the Russian Empire.\n",
    "        Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867. \n",
    "        Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.\"\"\"\n",
    "question = \"Where was Marie Curie's doctoral advisor Gabriel Lippmann born?\"\n",
    "\n",
    "result = question_answerer(question=question, context=context)\n",
    "print(f\"Answer: '{result['answer']}'\\n Score: {round(result['score'], 4)},\\n start token: {result['start']}, end token: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ea652-1d7e-4a3f-aed8-4b630c45d606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7baa2766-93cd-4634-b88a-761e422ae920",
   "metadata": {},
   "source": [
    "Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376d5bfe-dfd8-49c9-b6f6-230304695d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-large-4096-finetuned-triviaqa were not used when initializing LongformerForQuestionAnswering: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bonnevoie\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LongformerForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# setup the tokenizer and the model\n",
    "model_name = \"allenai/longformer-large-4096-finetuned-triviaqa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = LongformerForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# context and question\n",
    "context = \"\"\"Gabriel Lippmann, who supervised Marie Curie's doctoral research, was born in Bonnevoie, Luxembourg. \n",
    "        Marie Curie was born in Warsaw, Poland in what was then the Kingdom of Poland, part of the Russian Empire.\n",
    "        Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867. \n",
    "        Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.\"\"\"\n",
    "question = \"Where was Marie Curie's doctoral advisor Gabriel Lippmann born?\"\n",
    "\n",
    "# encode the question and the context\n",
    "encoded_input = tokenizer(question, context, return_tensors=\"pt\")\n",
    "input_ids = encoded_input[\"input_ids\"]\n",
    "\n",
    "# Generate the output masks\n",
    "outputs = model(input_ids)\n",
    "# find the beginning and end index of the answer in the encoded input\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits)\n",
    "\n",
    "# Convert the input ids to tokens\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "# extract the answer tokens and decode it\n",
    "answer_tokens = all_tokens[start_idx : end_idx + 1]\n",
    "answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88608c-490a-489d-97d0-d4d09950d303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a132c8ca-d8d6-42ff-9b85-537d7f92f8b9",
   "metadata": {},
   "source": [
    "Phi-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f55f4f83-f6c7-4970-bb81-e66d3f0d1732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.20s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e62ca7ce-d31f-415b-ab68-642473e590e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Taylor series expansion of a function f(x) about a point x=a is given by:\n",
      "\n",
      "f(x) = f(a) + f'(a)(x-a) + f''(a)(x-a)^2/2! + f'''(a)(x-a)^3/3! +...\n",
      "\n",
      "For the function sin(x) + ln(x), we need to find the derivatives and evaluate them at x=a.\n",
      "\n",
      "First, let's find the derivatives of sin(x) and ln(x):\n",
      "\n",
      "1. sin(x):\n",
      "   f(x) = sin(x)\n",
      "   f'(x) = cos(x)\n",
      "   f''(x) = -sin(x)\n",
      "   f'''(x) = -cos(x)\n",
      "  ...\n",
      "\n",
      "2. ln(x):\n",
      "   f(x) = ln(x)\n",
      "   f'(x) = 1/x\n",
      "   f''(x) = -1/x^2\n",
      "   f'''(x) = 2/x^3\n",
      "  ...\n",
      "\n",
      "Now, let's evaluate these derivatives at x=a:\n",
      "\n",
      "1. sin(a):\n",
      "   f(a) = sin(a)\n",
      "   f'(a) = cos(a)\n",
      "   f''(a) = -sin(a)\n",
      "   f'''(a) = -cos(a)\n",
      "  ...\n",
      "\n",
      "2. ln(a):\n",
      "   f(a) = ln(a)\n",
      "   f'(a) = 1/a\n",
      "   f''(a) = -1/a^2\n",
      "   f'''(a) = 2/a^3\n",
      "  ...\n",
      "\n",
      "Now, we can write the Taylor series expansion of sin(x) + ln(x) about x=a:\n",
      "\n",
      "sin(x) + ln(x) = (sin(a) + ln(a)) + (cos(a)(x-a) + (1/a)(x-a)) + (-sin(a)(x-a)^2/2! + (-1/a^2)(x-a)^2/2!) + (-cos(a)(x-a)^3/3! + (2/a^3)(x-a)^3/3!) +...\n",
      "\n",
      "This is the Taylor series expansion of sin(x) + ln(x) about x=a.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the Taylor series expansion of sin(x) + ln(x) about a point x=a?\"},\n",
    "    #{\"role\": \"user\", \"content\": \"What is the Taylor series expansion of sin(x) + 1/cos(x) about a point x=a?\"},\n",
    "]\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78643249-65ff-41e0-ba6d-39212f2b3047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Taylor series expansion of a function f(x) about a point x=a is given by:\n",
      "\n",
      "f(x) = f(a) + f'(a)(x-a) + f''(a)(x-a)^2/2! + f'''(a)(x-a)^3/3! +...\n",
      "\n",
      "First, let's find the Taylor series expansion of sin(x) and 1/cos(x) separately about x=a.\n",
      "\n",
      "For sin(x), the derivatives are:\n",
      "sin'(x) = cos(x)\n",
      "sin''(x) = -sin(x)\n",
      "sin'''(x) = -cos(x)\n",
      "sin''''(x) = sin(x)\n",
      "...\n",
      "\n",
      "The Taylor series expansion of sin(x) about x=a is:\n",
      "sin(x) = sin(a) + cos(a)(x-a) - sin(a)(x-a)^2/2! - cos(a)(x-a)^3/3! + sin(a)(x-a)^4/4! +...\n",
      "\n",
      "For 1/cos(x), the derivatives are:\n",
      "(1/cos(x))' = sin(x)/cos^2(x)\n",
      "(1/cos(x))'' = (cos(x) + sin^2(x))/cos^3(x)\n",
      "(1/cos(x))''' = (-2cos(x)sin(x) + 3sin^2(x))/cos^4(x)\n",
      "...\n",
      "\n",
      "The Taylor series expansion of 1/cos(x) about x=a is:\n",
      "1/cos(x) = 1/cos(a) + (sin(a)/cos^2(a))(x-a) + (cos(a)(sin^2(a) - 1)/cos^3(a))(x-a)^2/2! + (2cos(a)(sin^3(a) - 3sin(a))/cos^4(a))(x-a)^3/3! +...\n",
      "\n",
      "Now, we can find the Taylor series expansion of sin(x) + 1/cos(x) by adding the two series:\n",
      "\n",
      "sin(x) + 1/cos(x) = (sin(a) + 1/cos(a)) + (cos(a) + sin(a)/cos^2(a))(x-a) - (sin(a)(x-a)^2/2! + 1/cos^3(a)(x-a)^2/2!) +...\n",
      "\n",
      "This is the Taylor series expansion of sin(x) + 1/cos(x) about x=a.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    #{\"role\": \"user\", \"content\": \"What is the Taylor series expansion of sin(x) + ln(x) about a point x=a ?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the Taylor series expansion of sin(x) + 1/cos(x) about a point x=a?\"},\n",
    "]\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4219171-a2a3-47dd-b440-0ce56c64f834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb73ef3f-ea03-41f3-b8ec-9704e7f49df3",
   "metadata": {},
   "source": [
    "DistilRoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc2739f8-65c2-4098-96c6-539560554943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: \"there is a shortage of capital, and we need extra financing\"\n",
      "Sentiment: 'negative'\n",
      " Score: 0.666\n",
      "\n",
      "Input sentence: \"growth is strong and we have plenty of liquidity\"\n",
      "Sentiment: 'positive'\n",
      " Score: 0.9996\n",
      "\n",
      "Input sentence: \"there are doubts about our finances\"\n",
      "Sentiment: 'neutral'\n",
      " Score: 0.6857\n",
      "\n",
      "Input sentence: \"profits are flat\"\n",
      "Sentiment: 'neutral'\n",
      " Score: 0.9999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "model_name = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3, device_map=\"cuda\")\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sentences = [\"there is a shortage of capital, and we need extra financing\",  \n",
    "             \"growth is strong and we have plenty of liquidity\", \n",
    "             \"there are doubts about our finances\", \n",
    "             \"profits are flat\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    result = sentiment_analyzer(sentence)\n",
    "    print(f\"Input sentence: \\\"{sentence}\\\"\")\n",
    "    print(f\"Sentiment: '{result[0]['label']}'\\n Score: {round(result[0]['score'], 4)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ace03-e55f-400e-a7f7-f9ba4b5a8c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "104018b4-0afe-418c-b166-f1430a843fa8",
   "metadata": {},
   "source": [
    "FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c76837c7-65ab-43d4-a12f-c97c89a29830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: \"there is a shortage of capital, and we need extra financing\"\n",
      "Sentiment: 'Negative'\n",
      " Score: 0.9966\n",
      "\n",
      "Input sentence: \"growth is strong and we have plenty of liquidity\"\n",
      "Sentiment: 'Positive'\n",
      " Score: 1.0\n",
      "\n",
      "Input sentence: \"there are doubts about our finances\"\n",
      "Sentiment: 'Negative'\n",
      " Score: 1.0\n",
      "\n",
      "Input sentence: \"profits are flat\"\n",
      "Sentiment: 'Neutral'\n",
      " Score: 0.9889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3, device_map=\"cuda\")\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sentences = [\"there is a shortage of capital, and we need extra financing\",  \n",
    "             \"growth is strong and we have plenty of liquidity\", \n",
    "             \"there are doubts about our finances\", \n",
    "             \"profits are flat\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    result = sentiment_analyzer(sentence)\n",
    "    print(f\"Input sentence: \\\"{sentence}\\\"\")\n",
    "    print(f\"Sentiment: '{result[0]['label']}'\\n Score: {round(result[0]['score'], 4)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c0119-0dd9-45f7-8ebf-aa1355f1f866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe18610-a3ba-4cac-9671-debd98411b0b",
   "metadata": {},
   "source": [
    "BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "453acc83-cf50-47f3-a9e9-6292be322214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec2e91d4-c774-43f8-8168-161462b47170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.\n"
     ]
    }
   ],
   "source": [
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066c561-ecf8-4012-9988-3e263718f7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5680d211-07b1-40ef-929e-5e79f9c4ef48",
   "metadata": {},
   "source": [
    "Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e887866-5915-4b57-8784-f72208d802fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A New York woman who has been married 10 times has been charged with marriage fraud.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    " \n",
    "inputs = tokenizer(ARTICLE, max_length=1024, return_tensors=\"pt\")\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "\n",
    "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce9292-a84d-4880-b98b-f89995e5371b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5f05904-5b3a-4d84-a332-a0e07969dd79",
   "metadata": {},
   "source": [
    "Contriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3edf2080-73f8-43a9-a815-cde07cf5eb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9390654563903809, 1.1304867267608643, 1.0473244190216064, 1.0094892978668213]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"facebook/contriever\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "query = [\"Where was Marie Curie born?\"]\n",
    "\n",
    "docs = [\n",
    "    \"Gabriel Lippmann, who supervised Marie Curie's doctoral research, was born in Bonnevoie, Luxembourg.\",\n",
    "    \"Marie Curie was born in Warsaw, in what was then the Kingdom of Poland, part of the Russian Empire\",\n",
    "    \"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\",\n",
    "    \"Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.\"\n",
    "    ]\n",
    "\n",
    "corpus = query + docs\n",
    "\n",
    "# Apply tokenizer\n",
    "inputs = tokenizer(corpus, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Mean pooling\n",
    "def mean_pooling(token_embeddings, mask):\n",
    "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "    return sentence_embeddings\n",
    "embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
    "\n",
    "score = [0]*len(docs)\n",
    "for i in range(len(docs)):\n",
    "    score[i] = (embeddings[0] @ embeddings[i+1]).item()\n",
    "\n",
    "print(score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66db8812-488b-4684-8d8b-034a1c4aa6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant document to the query \" Where was Marie Curie born? \" is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Marie Curie was born in Warsaw, in what was then the Kingdom of Poland, part of the Russian Empire'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most relevant document to the query \\\"\", query[0], \"\\\" is\")\n",
    "docs[score.index(max(score))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5cfe2b-d3c6-440d-a46f-dd1384a183ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
