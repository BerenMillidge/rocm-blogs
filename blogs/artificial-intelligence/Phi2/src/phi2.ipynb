{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe4e9e0-4cb4-4919-a180-a587610ed1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate einops datasets\n",
    "!pip install --upgrade SQLAlchemy==1.4.46\n",
    "!pip install alembic==1.4.1\n",
    "!pip install numpy==1.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f8d49a7-f98f-4408-9520-81c75d89700a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.94s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d7ebbf4-3436-4e49-9686-8eaf1417162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(raw_input):\n",
    "    start_time = time.time()\n",
    "    inputs = tokenizer(raw_input, return_tensors=\"pt\", return_attention_mask=False)\n",
    "    outputs = model.generate(**inputs, max_length=500)\n",
    "    print(f\"Generated in {time.time() - start_time: .2f} seconds\")\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    # cut off at endoftext token\n",
    "    if '<|endoftext|>' in text:\n",
    "        index = text.index('<|endoftext|>') \n",
    "    else:\n",
    "        index = len(text) \n",
    "    text = text[:index] \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "351c68c2-6ee1-46e8-98f5-98d8cf54371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated in  19.42 seconds\n",
      " \n",
      "Given an integer array nums, return all the triplets [nums[i], nums[j], nums[k]] such that i!= j, i!= k, and j!= k, and nums[i] + nums[j] + nums[k] == 0.\n",
      "\n",
      "Notice that the solution set must not contain duplicate triplets.\n",
      "\n",
      "Example 1:\n",
      "\n",
      "Input: nums = [-1,0,1,2,-1,-4]\n",
      "Output: [[-1,-1,2],[-1,0,1]]\n",
      "Example 2:\n",
      "\n",
      "Input: nums = []\n",
      "Output: []\n",
      " \n",
      "\n",
      "Constraints:\n",
      "\n",
      "0 <= nums.length <= 3000\n",
      "-10^4 <= nums[i] <= 10^4\n",
      "\"\"\"\n",
      "\n",
      "class Solution:\n",
      "    def threeSum(self, nums: List[int]) -> List[List[int]]:\n",
      "        nums.sort()\n",
      "        res = []\n",
      "        for i in range(len(nums)):\n",
      "            if i > 0 and nums[i] == nums[i-1]:\n",
      "                continue\n",
      "            l, r = i+1, len(nums)-1\n",
      "            while l < r:\n",
      "                s = nums[i] + nums[l] + nums[r]\n",
      "                if s < 0:\n",
      "                    l += 1\n",
      "                elif s > 0:\n",
      "                    r -= 1\n",
      "                else:\n",
      "                    res.append([nums[i], nums[l], nums[r]])\n",
      "                    while l < r and nums[l] == nums[l+1]:\n",
      "                        l += 1\n",
      "                    while l < r and nums[r] == nums[r-1]:\n",
      "                        r -= 1\n",
      "                    l += 1\n",
      "                    r -= 1\n",
      "        return res\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = ''' \n",
    "Given an integer array nums, return all the triplets [nums[i], nums[j], nums[k]] such that i != j, i != k, and j != k, and nums[i] + nums[j] + nums[k] == 0.\n",
    "\n",
    "Notice that the solution set must not contain duplicate triplets.\n",
    "'''\n",
    "print(run_inference(raw_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad6e0b3-3313-473f-bbf5-97b3c362ab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated in  19.46 seconds\n",
      "\n",
      "Summarize the paper \"Attention Is All You Need\". \n",
      "## INPUT\n",
      "\n",
      "##OUTPUT\n",
      "The paper \"Attention Is All You Need\" proposes a novel neural network architecture called Transformer, which uses self-attention mechanisms to encode and decode sequences of data. The paper shows that Transformer outperforms existing models on various natural language processing tasks, such as machine translation, text summarization, and question answering. The paper also introduces the concept of attention, which allows the model to focus on relevant parts of the input and output, and to learn from the context of the data. The paper demonstrates that attention can be implemented efficiently and effectively using a single layer of trainable parameters, without the need for recurrent or convolutional layers. The paper also provides empirical evidence and theoretical analysis to support the effectiveness of attention in Transformer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = '''\n",
    "Summarize the paper \"Attention Is All You Need\". \n",
    "'''\n",
    "print(run_inference(raw_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "289d667e-7725-4efe-85e1-0392f4b14e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated in  18.17 seconds\n",
      "\n",
      "Instruct: Explain the joke below\n",
      "Q: Why did Beethoven get rid of all of his chickens?\n",
      "A: All they ever said was, “Bach, Bach, Bach!”.\n",
      "Output:\n",
      "The joke is a play on words. The expression “Bach, Bach, Bach” is a reference to the musical composition of Johann Sebastian Bach. The joke suggests that Beethoven was tired of his chickens constantly saying the same thing, implying that he wanted to get rid of them because they were too repetitive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = '''\n",
    "Instruct: Explain the joke below\n",
    "Q: Why did Beethoven get rid of all of his chickens?\n",
    "A: All they ever said was, “Bach, Bach, Bach!”.\n",
    "Output:\n",
    "'''\n",
    "print(run_inference(raw_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4f65a9-7286-4c1d-bc07-6ff00434fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated in  18.36 seconds\n",
      "\n",
      "Instruct: Explain the joke below\n",
      "Q: What do the Eiffel Tower and wood ticks have in common?\n",
      "A: They are both Paris sites/parasites!\n",
      "Output:\n",
      "The joke is based on a pun on the phrase \"Paris sites\" which is a play on the phrase \"Paris parasites\". The joke is a humorous comparison between the Eiffel Tower and wood ticks, suggesting that they are both located in Paris and can be considered as parasites.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = '''\n",
    "Instruct: Explain the joke below\n",
    "Q: What do the Eiffel Tower and wood ticks have in common?\n",
    "A: They are both Paris sites/parasites!\n",
    "Output:\n",
    "'''\n",
    "print(run_inference(raw_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f62f30d-657e-4d2e-8544-b9d1347f3ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated in  19.35 seconds\n",
      "\n",
      "Instruct: Write a detailed dialog between two physicists in Shakespearean english\n",
      "Output:\n",
      "Physicist 1: \"Good morrow, my dear friend! I have been pondering the mysteries of the universe, and I seek your wisdom.\"\n",
      "Physicist 2: \"Ah, thou art a seeker of truth! Pray tell, what enigma has captured thy mind?\"\n",
      "Physicist 1: \"I have been contemplating the nature of light, and its duality as both particle and wave. It is a perplexing concept indeed.\"\n",
      "Physicist 2: \"Ah, light, the very essence of illumination! It dances upon the stage of existence, revealing the secrets of the cosmos.\"\n",
      "Physicist 1: \"Indeed, but how can we reconcile its particle-like behavior with its wave-like properties? It defies logic!\"\n",
      "Physicist 2: \"Ah, my friend, logic is but a mere tool in our quest for understanding. We must embrace the beauty of uncertainty and explore the depths of the unknown.\"\n",
      "Physicist 1: \"You speak wise words, my friend. Let us embark on this journey together, unraveling the mysteries of the universe one photon at a time.\"\n",
      "Physicist 2: \"Indeed, let us delve into the realm of quantum mechanics, where the laws of classical physics crumble, and new wonders await.\"\n",
      "Physicist 1: \"And so, we shall venture forth, armed with our knowledge and curiosity, seeking the truth that lies hidden within the fabric of reality.\"\n",
      "Physicist 2: \"To the stars and beyond, my friend! May our quest for knowledge illuminate the path ahead.\"\n",
      "Physicist 1: \"To the stars and beyond!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = '''\n",
    "Instruct: Write a detailed dialog between two physicists in Shakespearean english\n",
    "Output:\n",
    "'''\n",
    "print(run_inference(raw_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8ced699-cbd9-4d29-9c1d-852af2b3d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated in  18.45 seconds\n",
      "\n",
      "Instruct: Create a function in Python that calculates the square root of a number using the 'advancedmath' package\n",
      "Output:\n",
      "```\n",
      "import advancedmath\n",
      "\n",
      "def calculate_square_root(number):\n",
      "    return advancedmath.sqrt(number)\n",
      "\n",
      "number = 25\n",
      "result = calculate_square_root(number)\n",
      "print(result)\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = '''\n",
    "Instruct: Create a function in Python that calculates the square root of a number using the 'advancedmath' package\n",
    "Output:\n",
    "'''\n",
    "print(run_inference(raw_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a4a3816-e283-4a20-8930-0429373385ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated in  18.45 seconds\n",
      "\n",
      "Summarize the paper \"LoRA: Low-Rank Adaptation of Large Language Models\" and explain the method in details. \n",
      "A: The paper \"LoRA: Low-Rank Adaptation of Large Language Models\" proposes a method for adapting large language models (LLMs) to specific tasks by reducing their size and complexity. The method, called Low-Rank Adaptation (LoRA), involves projecting the input text onto a lower-dimensional space using a low-rank matrix factorization (LRMF) technique. The resulting low-rank representation is then used to train a new LLM on the reduced data, which is found to perform well on the target task. The paper evaluates the effectiveness of LoRA on several benchmark tasks, including text classification, question answering, and machine translation, and shows that it achieves state-of-the-art performance while significantly reducing the computational cost of training LLMs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = '''\n",
    "Summarize the paper \"LoRA: Low-Rank Adaptation of Large Language Models\" and explain the method in details. \n",
    "'''\n",
    "print(run_inference(raw_inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
