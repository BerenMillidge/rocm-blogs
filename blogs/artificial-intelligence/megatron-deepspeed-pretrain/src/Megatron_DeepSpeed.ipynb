{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LpLGfqiV7kA"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers import set_seed\n",
        "import torch\n",
        "\n",
        "path0 = \"/home/aac/Megatron-DeepSpeed/examples_deepspeed/rebase/output/checkpoint/gpt_0.125B_tok300B_lr6.0e-4_min1.0e-6_w3000M_d300B_cosine_gbs256_mbs2_g8_z1_mp2_pp2_seed1234_rebase/HF/global_step2000/\"\n",
        "path1 = \"/home/aac/Megatron-DeepSpeed/examples_deepspeed/rebase/output/checkpoint/gpt_0.125B_tok300B_lr6.0e-4_min1.0e-6_w3000M_d300B_cosine_gbs256_mbs2_g8_z1_mp2_pp2_seed1234_rebase/HF/global_step8000/\"\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = GPT2Tokenizer(vocab_file='/home/aac/Megatron-DeepSpeed/dataset/gpt2-vocab.json', merges_file='/home/aac/Megatron-DeepSpeed/dataset/gpt2-merges.txt')\n",
        "model0 = GPT2LMHeadModel.from_pretrained(path0, pad_token_id=tokenizer.eos_token_id).to(torch_device)\n",
        "model1 = GPT2LMHeadModel.from_pretrained(path1, pad_token_id=tokenizer.eos_token_id).to(torch_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LmYiysuZobI",
        "outputId": "809ed699-0449-4640-9960-e727d19fdea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output with checkpoint from 2000 iterations:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I like to play golf. Today is a sunny day and I plan to work and get to work with my team. I think that I can make money but I make the effort to get to see this.\n",
            "I know how it works, but I do think that will\n",
            "1: I like to play golf. Today is a sunny day and I plan to go to the side of my life. It’s really simple! We have been there for a couple of days to try our training program. I have heard the video out there, I think\n",
            "2: I like to play golf. Today is a sunny day and I plan to get along that summer.\n",
            "A great weekend and a good one can be prepared. I'm a great place to try. It's fun to go and give you the chance to get along with me\n",
            "\n",
            "Output with checkpoint from 8000 iterations:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I like to play golf. Today is a sunny day and I plan to play some golf in the evening. I have not played my other tournaments until this morning.\n",
            "1: I like to play golf. Today is a sunny day and I plan to play the whole week of golf. I will be playing in the backyards to play golf.\n",
            "If you are still interested in playing the “American Association” Tournament, please don't hesitate\n",
            "2: I like to play golf. Today is a sunny day and I plan to get there on Monday morning.\n",
            "You’ll notice me playing in the backyard. My dad bought me the equipment, so I could throw it at home. When we went out to dinner we\n"
          ]
        }
      ],
      "source": [
        "#For more knowledge on how to fine tune the text generation process,\n",
        "#please visit this technical blog: https://huggingface.co/blog/how-to-generate\n",
        "\n",
        "# Encode context the generation is conditioned on.\n",
        "model_inputs = tokenizer('I like to play golf. Today is a sunny day and I plan to', return_tensors='pt').to(torch_device)\n",
        "\n",
        "# set seed to reproduce results. Feel free to change the seed to get different results.\n",
        "set_seed(1)\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model0.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output with checkpoint from 2000 iterations:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model1.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"\\nOutput with checkpoint from 8000 iterations:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFsw98oBc_2w",
        "outputId": "950be15a-2d5e-4987-b1f8-3fb4064ce46c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output with checkpoint from 2000 iterations:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I like to play golf. Today is a sunny day and I plan to work and get to work with my team. I think that I can make money but I make the effort to get to see this.\n",
            "I know how it works, but I do think that will\n",
            "1: I like to play golf. Today is a sunny day and I plan to go to the side of my life. It’s really simple! We have been there for a couple of days to try our training program. I have heard the video out there, I think\n",
            "2: I like to play golf. Today is a sunny day and I plan to get along that summer.\n",
            "A great weekend and a good one can be prepared. I'm a great place to try. It's fun to go and give you the chance to get along with me\n",
            "\n",
            "Output with checkpoint from 8000 iterations:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I like to play golf. Today is a sunny day and I plan to play some golf in the evening. I have not played my other tournaments until this morning.\n",
            "1: I like to play golf. Today is a sunny day and I plan to play the whole week of golf. I will be playing in the backyards to play golf.\n",
            "If you are still interested in playing the “American Association” Tournament, please don't hesitate\n",
            "2: I like to play golf. Today is a sunny day and I plan to get there on Monday morning.\n",
            "You’ll notice me playing in the backyard. My dad bought me the equipment, so I could throw it at home. When we went out to dinner we\n"
          ]
        }
      ],
      "source": [
        "model3 = GPT2LMHeadModel.from_pretrained('jiagaoxiang/gpt3-125M-2000iter', pad_token_id=tokenizer.eos_token_id).to(torch_device)\n",
        "model4 = GPT2LMHeadModel.from_pretrained('jiagaoxiang/gpt3-125M-8000iter', pad_token_id=tokenizer.eos_token_id).to(torch_device)\n",
        "\n",
        "model_inputs = tokenizer('I like to play golf. Today is a sunny day and I plan to', return_tensors='pt').to(torch_device)\n",
        "\n",
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "set_seed(1)\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model3.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output with checkpoint from 2000 iterations:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model4.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"\\nOutput with checkpoint from 8000 iterations:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}