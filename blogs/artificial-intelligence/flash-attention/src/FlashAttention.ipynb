{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b7437-95e7-4537-bffb-55f084b3b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, is_causal=False, dropout_p=0.0, scale=None):\n",
    "    \"\"\"\n",
    "    Computes the scaled dot product attention between query, key, and value tensors in PyTorch eager mode.\n",
    "\n",
    "    Args:\n",
    "        query (torch.Tensor): The query tensor of shape (batch_size, n_heads, seq_len, hidden_dim).\n",
    "        key (torch.Tensor): The key tensor of shape (batch_size, n_heads, seq_len, hidden_dim).\n",
    "        value (torch.Tensor): The value tensor of shape (batch_size, n_heads, seq_len, hidden_dim).\n",
    "        attn_mask (torch.Tensor, optional): The attention mask tensor of shape (batch_size, n_heads, seq_len, seq_len). Defaults to None.\n",
    "        is_causal (bool, optional): Whether to apply a causal attention mask. Defaults to False.\n",
    "        dropout_p (float, optional): The dropout probability. Defaults to 0.\n",
    "        scale (float, optional): The scale factor for the dot product. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The output tensor of shape (batch_size, n_heads, seq_len, hidden_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the scale factor\n",
    "    scale_factor = 1 / np.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_weight = (query @ key.transpose(-2, -1) * scale_factor)\n",
    "    \n",
    "    # Create the attention mask\n",
    "    attn_mask = torch.ones(query.shape[0], query.shape[1], query.shape[2], query.shape[2], dtype=torch.bool, device=device).tril(diagonal=0) if is_causal else attn_mask\n",
    "    attn_weight = attn_weight.masked_fill_(~attn_mask, -torch.inf) if attn_mask is not None else attn_weight\n",
    "      \n",
    "    # Compute the scaled dot product attention\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=False)\n",
    "\n",
    "    return attn_weight @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c56235-6cbb-4b05-8b54-5714d4fd3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 64\n",
    "num_heads = 32\n",
    "embed_dim = 128\n",
    "dtype = torch.float16\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "query = torch.rand(batch_size, num_heads, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "key = torch.rand(batch_size, num_heads, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "value = torch.rand(batch_size, num_heads, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "eager = scaled_dot_product_attention(query, key, value, is_causal=True)\n",
    "flash = F.scaled_dot_product_attention(query, key, value, is_causal=True)\n",
    "assert torch.allclose(eager, flash, rtol=1e-03,atol=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d50404-f71d-46e6-8543-dc6645891708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench_attention(seq_len, flash=False, num_repeats=256):\n",
    "    \"\"\"\n",
    "    Measures the average time (in ms) required to compute multi-head attention for sequences of a given length.\n",
    "\n",
    "    Args:\n",
    "        seq_len (int): The length of the input sequence.\n",
    "        flash (bool, optional): Whether to use the FlashAttention algorithm. Defaults to False.\n",
    "        num_repeats (int, optional): The number of times to repeat the attention computation for timing purposes. Defaults to 256.\n",
    "\n",
    "    Returns:\n",
    "        float: The average time (in ms) required to compute multi-head attention for sequences of length seq_len.\n",
    "    \"\"\"\n",
    "    \n",
    "    if flash:\n",
    "        mha = F.scaled_dot_product_attention\n",
    "    else:\n",
    "        mha = scaled_dot_product_attention\n",
    "        \n",
    "    query = torch.rand(batch_size, num_heads, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    key = torch.rand(batch_size, num_heads, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    value = torch.rand(batch_size, num_heads, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(4):\n",
    "        _ = mha(query, key, value, is_causal=True)\n",
    "\n",
    "    start.record()\n",
    "    for _ in range(num_repeats):\n",
    "        _ = mha(query, key, value, is_causal=True)   \n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return start.elapsed_time(end) / num_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78f743-3f8c-4a99-a7ca-1e4e3fb3dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = np.arange(256,4096,64)\n",
    "flash = np.zeros(context_len.shape)\n",
    "standard = np.zeros(context_len.shape)\n",
    "\n",
    "for i,l in enumerate(tqdm(context_len)):\n",
    "    flash[i] = bench_attention(l,flash=True)\n",
    "    standard[i] = bench_attention(l,flash=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bade882-be85-464a-a6cd-54f7999d81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(context_len, standard/flash)\n",
    "plt.xlabel('Sequence length')\n",
    "plt.ylabel('Speedup')\n",
    "plt.title('FlashAttention vs. Standard Attention, head_size=128, n_heads=32, bs=1') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c2e1338-9ac5-4c01-84f5-d8fc9aeebac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench_llm(seq_len, model_name, max_new_tokens=128, flash=False, num_repeats=256):\n",
    "    \"\"\"\n",
    "    Benchmark the end-to-end latency of a large language model (LLM) in Hugging Face, with FlashAttention enabled or disabled.\n",
    "\n",
    "    Args:\n",
    "        seq_len (int): Length of the input sequence.\n",
    "        model_name (str): Name of the pre-trained LLM to use.\n",
    "        max_new_tokens (int, optional): Maximum number of new tokens to generate. Defaults to 128.\n",
    "        flash (bool, optional):\n",
    "        Whether to use flash attention mechanism (if supported by the model).\n",
    "        num_repeats (int, optional):\n",
    "        Number of times to repeat the inference for averaging. Defaults to 256.\n",
    "\n",
    "    Returns:\n",
    "        float: The average end-to-end latency in seconds.\n",
    "    \"\"\"\n",
    "\n",
    "    if flash:\n",
    "        mech = \"flash_attention_2\"\n",
    "    else:\n",
    "        mech = \"eager\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        attn_implementation=mech,\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "    \n",
    "    token_ids = {\n",
    "                    'input_ids': torch.randint(1, 10000, size=(1, seq_len), device='cuda'),\n",
    "                    'attention_mask': torch.ones(1, seq_len, device='cuda')\n",
    "    }\n",
    "    \n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(4):\n",
    "        _ = model.generate(**token_ids, max_new_tokens=max_new_tokens, pad_token_id=pad_token_id)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start.record()\n",
    "    for _ in range(num_repeats):\n",
    "        _ = model.generate(**token_ids, max_new_tokens=max_new_tokens, pad_token_id=pad_token_id) \n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return start.elapsed_time(end) / num_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3e8e4-5ec3-4e43-9c6e-c9125ceb212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = np.array([512, 1024, 2048])\n",
    "model_names = ['mistralai/Mistral-7B-v0.1', 'NousResearch/Meta-Llama-3-8B', 'microsoft/phi-2']\n",
    "prefill_flash = np.zeros((len(model_names),len(seq_lens)))\n",
    "prefill_standard = np.zeros((len(model_names),len(seq_lens)))\n",
    "for j, model_name in enumerate(tqdm(model_names)):\n",
    "    for i, seq_len in enumerate(seq_lens):\n",
    "        prefill_flash[j,i] = bench_llm(seq_len, model_name=model_name, max_new_tokens=1, flash=True)\n",
    "        prefill_standard[j,i] = bench_llm(seq_len, model_name=model_name, max_new_tokens=1, flash=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00801c2-927c-4868-86e8-58fa3a53cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_prefill = prefill_standard/prefill_flash\n",
    "\n",
    "models = [\"Mistral-7B\", \"Llama3-8B\", \"Phi-2\"]\n",
    "avg_speedup = {\n",
    "    '512': speedup_prefill.T[0],\n",
    "    '1024': speedup_prefill.T[1],\n",
    "    '2048': speedup_prefill.T[2],\n",
    "}\n",
    "\n",
    "x = np.arange(len(avg_speedup))  \n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "for attribute, measurement in avg_speedup.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, fmt='%.2f', padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "ax.legend(loc='upper left', ncols=1)\n",
    "ax.set_xticks(x + width, models)\n",
    "ax.set_ylabel('Speedup')\n",
    "ax.set_title('Flash Attention vs Standard Attention Prefill Latency')\n",
    "plt.savefig('benchmark-llm.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
