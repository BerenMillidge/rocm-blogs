{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf000e5f-97f9-4068-b1a0-1ea7e5d2d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================= ROCm System Management Interface =========================\n",
      "=================================== Product Info ===================================\n",
      "GPU[0]\t\t: Card series: \t\tInstinct MI210\n",
      "GPU[0]\t\t: Card model: \t\t0x0c34\n",
      "GPU[0]\t\t: Card vendor: \t\tAdvanced Micro Devices, Inc. [AMD/ATI]\n",
      "GPU[0]\t\t: Card SKU: \t\tD67301V\n",
      "====================================================================================\n",
      "=============================== End of ROCm SMI Log ================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! rocm-smi --showproductname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt show rocm-libs -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c89b31-9d6b-4ef1-bde0-0f15a0652e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers accelerate einops datasets\n",
    "%pip install --upgrade SQLAlchemy==1.4.46\n",
    "%pip install -q alembic==1.4.1 numpy==1.23.4 grpcio-status==1.33.2 protobuf==3.19.6 \n",
    "%pip install -q evaluate rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5116741-21fb-460f-bfed-ecff6ee501c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9843af-03d2-4b56-964c-8b999b114bef",
   "metadata": {},
   "source": [
    "## Running inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "418a3546-fdf0-4818-8407-e21eec269832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [01:23<00:00, 16.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in  85.46 seconds\n",
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 4096)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 4096)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 64)\n",
      "            )\n",
      "            (layer_norm): FusedRMSNorm(torch.Size([4096]), eps=1e-06, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
      "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
      "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): FusedRMSNorm(torch.Size([4096]), eps=1e-06, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            )\n",
      "            (layer_norm): FusedRMSNorm(torch.Size([4096]), eps=1e-06, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
      "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
      "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): FusedRMSNorm(torch.Size([4096]), eps=1e-06, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): FusedRMSNorm(torch.Size([4096]), eps=1e-06, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 4096)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 64)\n",
      "            )\n",
      "            (layer_norm): FusedRMSNorm(torch.Size([4096]), eps=1e-06, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            )\n",
      "            (layer_norm): FusedRMSNorm(torch.Size([4096]), eps=1e-06, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
      "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
      "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): FusedRMSNorm(torch.Size([4096]), eps=1e-06, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            )\n",
      "            (layer_norm): FusedRMSNorm(torch.Size([4096]), eps=1e-06, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            )\n",
      "            (layer_norm): FusedRMSNorm(torch.Size([4096]), eps=1e-06, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
      "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
      "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): FusedRMSNorm(torch.Size([4096]), eps=1e-06, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): FusedRMSNorm(torch.Size([4096]), eps=1e-06, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model_checkpoint = \"google/flan-t5-xxl\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(f\"Loaded in {time.time() - start_time: .2f} seconds\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "829f791d-6f14-4681-bb25-82872fe170de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pour a cup of coffee into a mug. Add a tablespoon of milk. Add a pinch of sugar.']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"How to make milk coffee\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98b018b5-9c4f-421d-a1fd-4a0439013cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amy and Mark are going to see \"Stellar Odyssey\" on Saturday at 7 pm.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\" summarize: \n",
    "Amy: Hey Mark, have you heard about the new movie coming out this weekend?\n",
    "Mark: Oh, no, I haven't. What's it called?\n",
    "Amy: It's called \"Stellar Odyssey.\" It's a sci-fi thriller with amazing special effects.\n",
    "Mark: Sounds interesting. Who's in it?\n",
    "Amy: The main lead is Emily Stone, and she's fantastic in the trailer. The plot revolves around a journey to a distant galaxy.\n",
    "Mark: Nice! I'm definitely up for a good sci-fi flick. Want to catch it together on Saturday?\n",
    "Amy: Sure, that sounds great! Let's meet at the theater around 7 pm.\n",
    "\"\"\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded7a974-e1aa-4286-a430-75cf20256c95",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b149c71-7dc8-4c54-9bd0-27c769722cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in  412.36 seconds\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"google/flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(f\"Loaded in {time.time() - start_time: .2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3af5d6-0489-4df1-9e51-323e2e93f306",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d805a2cf-fa40-42c8-8384-b1945a73ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "raw_datasets = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bd630e8-6ce7-47ce-b707-6373cf862f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue: \n",
      "Gabby: How is you? Settling into the new house OK?\n",
      "Sandra: Good. The kids and the rest of the menagerie are doing fine. The dogs absolutely love the new garden. Plenty of room to dig and run around.\n",
      "Gabby: What about the hubby?\n",
      "Sandra: Well, apart from being his usual grumpy self I guess he's doing OK.\n",
      "Gabby: :-D yeah sounds about right for Jim.\n",
      "Sandra: He's a man of few words. No surprises there. Give him a backyard shed and that's the last you'll see of him for months.\n",
      "Gabby: LOL that describes most men I know.\n",
      "Sandra: Ain't that the truth! \n",
      "Gabby: Sure is. :-) My one might as well move into the garage. Always tinkering and building something in there.\n",
      "Sandra: Ever wondered what he's doing in there?\n",
      "Gabby: All the time. But he keeps the place locked.\n",
      "Sandra: Prolly building a portable teleporter or something. ;-)\n",
      "Gabby: Or a time machine... LOL\n",
      "Sandra: Or a new greatly improved Rabbit :-P\n",
      "Gabby: I wish... Lmfao!\n",
      "\n",
      "Summary:  Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending time in the garage or a shed.\n"
     ]
    }
   ],
   "source": [
    "print('Dialogue: ')\n",
    "print(raw_datasets['train']['dialogue'][100])\n",
    "print() \n",
    "print('Summary: ', raw_datasets['train']['summary'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8916152-dfea-4a3f-a8db-3397b74a770b",
   "metadata": {},
   "source": [
    "### Set up metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49d1e2e9-9d06-4475-9a54-cb244367f596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
      "Calculates average rouge scores for a list of hypotheses and references\n",
      "Args:\n",
      "    predictions: list of predictions to score. Each prediction\n",
      "        should be a string with tokens separated by spaces.\n",
      "    references: list of reference for each prediction. Each\n",
      "        reference should be a string with tokens separated by spaces.\n",
      "    rouge_types: A list of rouge types to calculate.\n",
      "        Valid names:\n",
      "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
      "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
      "        `\"rougeLsum\"`: rougeLsum splits text using `\"\n",
      "\"`.\n",
      "        See details in https://github.com/huggingface/datasets/issues/617\n",
      "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
      "    use_aggregator: Return aggregates if this is set to True\n",
      "Returns:\n",
      "    rouge1: rouge_1 (f1),\n",
      "    rouge2: rouge_2 (f1),\n",
      "    rougeL: rouge_l (f1),\n",
      "    rougeLsum: rouge_lsum (f1)\n",
      "Examples:\n",
      "\n",
      "    >>> rouge = evaluate.load('rouge')\n",
      "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
      "    >>> references = [\"hello there\", \"general kenobi\"]\n",
      "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "\"\"\", stored examples: 0)\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "metric = load(\"rouge\")\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0941bb8c-1cfe-4b28-8e20-104983b62a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Note that other metrics may not have a `use_aggregator` parameter\n",
    "    # and thus will return a list, computing a metric for each sentence.\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9ca73-82ff-4e5a-bf94-561500b386bb",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2951a504-d3d3-4414-9bd7-1a32b5141331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14732/14732 [00:01<00:00, 10988.08 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 11409.91 examples/s]\n",
      "Map: 100%|██████████| 818/818 [00:00<00:00, 12122.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2268c787-58cf-4542-8438-f2de1eaa6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-samsum\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d674d073-0216-4e1b-a120-1d9ff7774799",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09a8713b-be6d-4d58-9fb2-20b9b4549250",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "732d6d73-52b8-4cbc-a0d4-314a87cb8eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1842' max='1842' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1842/1842 04:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.865600</td>\n",
       "      <td>1.693111</td>\n",
       "      <td>43.408600</td>\n",
       "      <td>19.803800</td>\n",
       "      <td>36.157700</td>\n",
       "      <td>39.980700</td>\n",
       "      <td>16.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.817000</td>\n",
       "      <td>1.685713</td>\n",
       "      <td>43.451500</td>\n",
       "      <td>19.880100</td>\n",
       "      <td>36.228600</td>\n",
       "      <td>39.999100</td>\n",
       "      <td>16.797100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1842, training_loss=1.84107419489261, metrics={'train_runtime': 253.1745, 'train_samples_per_second': 116.378, 'train_steps_per_second': 7.276, 'total_flos': 4315642670825472.0, 'train_loss': 1.84107419489261, 'epoch': 2.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5489525f-28eb-405e-86a1-4b5856a4a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" summarize: \n",
    "Hannah: Hey, Mark, have you decided on your New Year's resolution yet?\n",
    "Mark: Yeah, I'm thinking of finally hitting the gym regularly. What about you?\n",
    "Hannah: I'm planning to read more books this year, at least one per month.\n",
    "Mark: That sounds like a great goal. Any particular genre you're interested in?\n",
    "Hannah: I want to explore more classic literature. Maybe start with some Dickens or Austen.\n",
    "Mark: Nice choice. I'll hold you to it. We can discuss our progress over coffee.\n",
    "Hannah: Deal! Accountability partners it is.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f4965cd-6030-4502-87f5-1619bfb1a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"flan-t5-small-finetuned-samsum/checkpoint-1500\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"flan-t5-small-finetuned-samsum/checkpoint-1500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e42ab6c6-cd28-48d5-b13b-582929ec159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb59dae9-14cc-4d90-a4b1-0742c80402d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "543dba08-7545-4c82-8c0b-3264e6480553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hannah is planning to read more books this year. Mark will hold Hannah to it.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
