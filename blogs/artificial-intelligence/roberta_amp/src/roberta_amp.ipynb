{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653251eb-b3b6-4b15-af77-bfaed31fc4d6",
   "metadata": {},
   "source": [
    "## 1.- Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665cb4b-5969-483d-b63e-1a8096533374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, set_seed\n",
    "import datasets\n",
    "import time\n",
    "\n",
    "set_seed(42)\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61560901-29ad-4349-9601-6885d51df300",
   "metadata": {},
   "source": [
    "### 1.1.- Load and Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62e506-0b1e-48c3-b638-6f76eeac04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset description can be found at https://huggingface.co/datasets/dair-ai/emotion\n",
    "\n",
    "# Load train validation and test splits\n",
    "train_data = datasets.load_dataset(\"dair-ai/emotion\", split=\"train\",trust_remote_code=True)\n",
    "validation_data = datasets.load_dataset(\"dair-ai/emotion\", split=\"validation\",trust_remote_code=True)\n",
    "test_data = datasets.load_dataset(\"dair-ai/emotion\", split=\"test\",trust_remote_code=True)\n",
    "\n",
    "# Show dataset number of examples and column names\n",
    "print(train_data)\n",
    "print(validation_data)\n",
    "print(test_data,'\\n')\n",
    "\n",
    "# Print the first instance and label on the train split\n",
    "print('Text:',train_data['text'][0], '| Label:', train_data['label'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e9758-76ce-4ea1-a565-8bf0f894efc6",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aef93a-aa7b-4e18-8067-819ecba65600",
   "metadata": {},
   "source": [
    "# 2.- Finetune with native Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18adcdcb-72ad-4f46-b7fc-86fa3eb8365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load train validation and test splits\n",
    "# Dataset description can be found at https://huggingface.co/datasets/dair-ai/emotion\n",
    "\n",
    "train_data = datasets.load_dataset(\"dair-ai/emotion\", split=\"train\",trust_remote_code=True)\n",
    "validation_data = datasets.load_dataset(\"dair-ai/emotion\", split=\"validation\",trust_remote_code=True)\n",
    "test_data = datasets.load_dataset(\"dair-ai/emotion\", split=\"test\",trust_remote_code=True)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(examples['text'], padding = 'max_length',return_tensors=\"pt\")\n",
    "\n",
    "# Apply tokenization to each split\n",
    "tokenized_train_data = train_data.map(tokenize_function, batched = True)\n",
    "tokenized_validation_data = validation_data.map(tokenize_function, batched = True)\n",
    "tokenized_test_data = test_data.map(tokenize_function, batched = True)\n",
    "\n",
    "# Set type to PyTorch tensors\n",
    "tokenized_train_data.set_format(type=\"torch\")\n",
    "tokenized_validation_data.set_format(type=\"torch\")\n",
    "tokenized_test_data.set_format(type=\"torch\")\n",
    "\n",
    "# Transform tokenized datasets to PyTorch dataloder\n",
    "train_loader = DataLoader(tokenized_train_data, batch_size = 32)\n",
    "validation_loader = DataLoader(tokenized_validation_data, batch_size = 32)\n",
    "test_loader = DataLoader(tokenized_test_data, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9400ea48-a9b0-4ce0-9e41-70a12c8f3eda",
   "metadata": {},
   "source": [
    "## 2.1 No mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5daa36-91c5-4c40-8f63-c3cd7b04e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Roberta model for sequence classification\n",
    "num_labels = 6 # The dair-ai/emotion contains 6 labels\n",
    "epochs = 3\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = num_labels)\n",
    "model.to(device)\n",
    "\n",
    "# Instantiate the optimizer with the given learning rate\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5)\n",
    "\n",
    "# Training Loop\n",
    "model.train()\n",
    "\n",
    "# Train the model\n",
    "torch.cuda.synchronize() # Wait for all kernels to finish\n",
    "start_time = time.time()    \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        inputs = {'input_ids':batch['input_ids'].to(model.device),\n",
    "                  'attention_mask':batch['attention_mask'].to(model.device),\n",
    "                  'labels':batch['label'].to(model.device)\n",
    "                 }\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        clear_output(wait=True)            \n",
    "        display(f'Epoch: {epoch+1}/{epochs}. Training Loss: {loss.item()}')\n",
    "\n",
    "    #Validation Loop\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    for batch in validation_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':batch['input_ids'].to(model.device),\n",
    "                      'attention_mask':batch['attention_mask'].to(model.device),\n",
    "                      'labels':batch['label'].to(model.device)\n",
    "                     }\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_loader)\n",
    "    \n",
    "    display(f'Validation Loss: {avg_val_loss}')\n",
    "\n",
    "torch.cuda.synchronize() # Wait for all kernels to finish\n",
    "training_time_regular = time.time() - start_time\n",
    "print(f'Mixed Precision False. Training time (s):{training_time_regular:.3f}')\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(f'./native_finetuned_roberta_mixed_precision_false')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1d48e-261e-4e22-af07-b063b3c79096",
   "metadata": {},
   "source": [
    "## 2.2.- Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1baffa-e8dc-4a44-a871-9832da8cb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def roberta_finetuned_performance_metrics(saved_model_path, tokenizer):\n",
    "\n",
    "    is_mixed_precision = saved_model_path.split('_')[-1]\n",
    "    model = RobertaForSequenceClassification.from_pretrained(saved_model_path)\n",
    "    model.to(device)\n",
    "\n",
    "    # return predictions\n",
    "    def inference(batch):        \n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        predictions = torch.argmax(outputs.logits,dim = -1).cpu().numpy()\n",
    "    \n",
    "        return {'predictions': predictions}\n",
    "\n",
    "\n",
    "    # Perform inference on test set\n",
    "    results = tokenized_test_data.map(inference, batched=True, batch_size = 32)\n",
    "    \n",
    "    # Extract predictions and true labels\n",
    "    predictions = results['predictions'].tolist()\n",
    "    true_labels = tokenized_test_data['label'].tolist()\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels,predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average = 'weighted')\n",
    "    \n",
    "    print(f'Model mixed precision: {is_mixed_precision}.\\nPrecision: {precision:.3f} | Recall: {recall:.3f}  | F1: {f1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907da65e-6f36-4192-b863-dc6ff4caa777",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = './native_finetuned_roberta_mixed_precision_false'    \n",
    "roberta_finetuned_performance_metrics(saved_model_path, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746bcc0e-b44b-4589-a426-eed89611aee7",
   "metadata": {},
   "source": [
    "## 2.3.- Using mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a261431-9436-4b64-bbbf-1f1c4c1c1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Roberta model for sequence classification\n",
    "num_labels = 6 # The dair-ai/emotion contains 6 labels\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = num_labels)\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5)\n",
    "\n",
    "# Instantiate gradient scaler\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Train the model\n",
    "torch.cuda.synchronize() # Wait for all kernels to finish\n",
    "model.train()\n",
    "\n",
    "start_time = time.time()  \n",
    "for epoch in range(epochs):\n",
    "    for batch in tqdm(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()        \n",
    "\n",
    "        inputs = {'input_ids':batch['input_ids'].to(model.device),\n",
    "                  'attention_mask':batch['attention_mask'].to(model.device),\n",
    "                  'labels':batch['label'].to(model.device)\n",
    "                 }\n",
    "        \n",
    "        #Use Automatic Mixed Precision\n",
    "        with torch.cuda.amp.autocast():        \n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        clear_output(wait=True)             \n",
    "        display(f'Epoch: {epoch+1}/{epochs}. Training Loss: {loss.item()}')        \n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_loader:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            inputs = {'input_ids':batch['input_ids'].to(model.device),\n",
    "                      'attention_mask':batch['attention_mask'].to(model.device),\n",
    "                      'labels':batch['label'].to(model.device)\n",
    "                     }\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss +=loss.item()\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_loader)\n",
    "    \n",
    "    display(f'Validation Loss: {avg_val_loss}')\n",
    "\n",
    "torch.cuda.synchronize() # Wait for all kernels to finish\n",
    "training_time_amp = time.time() - start_time\n",
    "print(f'Mixed Precision True. Training time (s):{training_time_amp:.3f}')\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(f'./native_finetuned_roberta_mixed_precision_true')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74597837-a92c-418a-8434-e3d6e2f3aaeb",
   "metadata": {},
   "source": [
    "## 2.4 Peformance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed10b0-49e6-4860-9676-e29b847956a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = './native_finetuned_roberta_mixed_precision_true'    \n",
    "roberta_finetuned_performance_metrics(saved_model_path, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612bcaf-df60-40a0-a721-4a0628912db8",
   "metadata": {},
   "source": [
    "# Appedix: Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab3ca9-0284-4122-ab7a-5948883ccd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "values = [training_time_regular, training_time_amp]\n",
    "labels = ['Regular Fine-tune', 'Mixed Precision Fine-tune']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x = labels,\n",
    "    y = values,\n",
    "    text = [f'{v:.2f}' for v in values],\n",
    "    textposition = 'auto',\n",
    "    marker_color = ['indianred', 'steelblue']\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title = 'Training time comparison',\n",
    "    xaxis_title = 'Training type',\n",
    "    yaxis_title = 'Seconds',\n",
    "    template = 'plotly_white',\n",
    "    height = 500    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab525f0-a9b2-4aaa-bab6-e8f3af2d87cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
