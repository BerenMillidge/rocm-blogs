{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cedce07c-3391-4401-a7cb-5eef54b3f360",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GPU Unleashed: Training Reinforcement Learning Agents with Stable Baselines3 on AMD GPU in Gymnasium Environment\n",
    "\n",
    "**Author:** Douglas Jia, AI Software Solutions \\\n",
    "**Read time:** 10 minutes\\\n",
    "**Last edited:** xx Jan 2024\n",
    "\n",
    "This blog will delve into the fundamentals of deep reinforcement learning, guiding you through a practical code example that utilizes an AMD GPU to train a Deep Q-Network (DQN) policy within the Gymnasium environment.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### What is reinforcement learning\n",
    "\n",
    "Reinforcement learning (RL) is a powerful paradigm in machine learning that revolves around the idea of training intelligent agents to make sequential decisions in dynamic environments. Unlike supervised learning, RL agents learn by interacting with an environment, receiving feedback in the form of rewards or penalties based on the actions they take. The fundamental goal of reinforcement learning is for an agent to learn a policy that represents a strategy that maps states to actions in a way that increases the expected reward over time. The learning process involves the agent exploring the environment, making decisions, observing outcomes, and adjusting its strategy based on the received feedback. This iterative cycle of exploration and exploitation allows RL agents to adapt to changing conditions, learn complex decision-making strategies, and, ultimately, optimize their behavior to achieve long-term objectives. Reinforcement learning finds applications in diverse fields, from robotics and game playing to finance and autonomous systems, where agents can autonomously learn and improve their decision-making abilities in complex and dynamic scenarios.\n",
    "\n",
    "### How does deep learning come into play\n",
    "\n",
    "In reinforcement learning, deep learning can address the challenges posed by high-dimensional state spaces and enhance function (i.e., state-action value function or policy) approximation. This is particularly crucial in environments with extensive or continuous state spaces, especially when represented by high-dimensional inputs like images. Neural networks, with their interconnected layers of neurons, excel at automatically learning hierarchical representations from raw data. This ability is vital for capturing intricate patterns necessary for decision-making in RL tasks. Deep RL algorithms, such as DQN or policy gradient methods, leverage the power of deep learning to generalize across diverse states, making them well-suited for real-world applications where traditional methods may struggle. The synergy of deep learning and RL enables the training of agents capable of mastering complex tasks in dynamic and diverse environments. Specifically, the input to a deep reinforcement learning system is typically observations from the environment, and the goal is to learn a policy (represented by a neural network) that outputs actions to maximize cumulative rewards over time. The learning process involves optimizing the parameters of the policy based on the feedback received from the environment in the form of rewards.\n",
    "\n",
    "If you want to dive deeper into the knowledge of deep RL, you can visit this [Deep Reinforcement Learning Course](https://huggingface.co/learn/deep-rl-course/unit0/introduction) hosted by Hugging Face.\n",
    "\n",
    "### What are Gymnasium and Stable Baselines3\n",
    "\n",
    "Imagine a virtual playground for AI athletes – that's Gymnasium! [Gymnasium](https://gymnasium.farama.org/) is a maintained fork of OpenAI’s Gym library. This open-source toolkit provides virtual environments, from balancing Cartpole robots to navigating Lunar Lander challenges. It's where your AI agents get to flex their muscles and learn by doing.\n",
    "\n",
    "But training these athletes takes a coach, one with experience and finesse. That's where [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/) comes in. This package empowers you with ready-made reinforcement learning algorithms, from DQN to Policy Gradients. Think of it as a toolbox full of proven training techniques, ready to unleash your AI agent's full potential within the Gym's diverse arenas.\n",
    "\n",
    "In essence, Gymnasium serves as the environment for the application of deep learning algorithms offered by Stable Baselines3 to learn and optimize policies. To enhance the efficiency of the training process, we harnessed the power of AMD GPUs, and in the code example below, we'll demonstrate the extent of acceleration achievable through this approach.\n",
    "\n",
    "## Code example on training a car racing agent\n",
    "\n",
    "### Implementation environment \n",
    "\n",
    "We implemented the code example with an AMD GPU on [AMD Accelerator Cloud](https://aac.amd.com/) with Pytorch 2.0.1 and ROCm 5.7.0. Refer to this [ROCm documentation page](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html) for a list of supported OS and hardware.\n",
    "\n",
    "To start the example, we need to install the python packages needed to run this code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "864a5809-bf89-47ff-a40b-33086f6b6595",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install stable-baselines3 gymnasium[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a75c7c-83eb-43cd-aafc-38222e136c5c",
   "metadata": {},
   "source": [
    "### Gymnasium environment\n",
    "\n",
    "We will use the `CarRacing-v2` environment with discrete action spaces in Gymnasium. For more detailed information about this environment, please refer to [the official documentation](https://gymnasium.farama.org/environments/box2d/car_racing/). The Car Racing environment in Gymnasium is a simulation designed for training reinforcement learning agents in the context of car racing. The primary goal of this environment is for the agent to navigate a track efficiently, considering factors such as speed, steering, and braking, while maximizing cumulative rewards. The state space in CarRacing includes visual observations represented as RGB images, providing the agent with information about the current state of the environment. The discrete action space consists of 5 actions: `0: do nothing`, `1: steer left`, `2: steer right`, `3: gas`, and `4: brake`, allowing the agent to control the car's movements. The reward function, based on official documentation, is \"-0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles visited in the track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\" and \"The car can also go outside the playfield - that is, far off the track, in which case it will receive -100 reward and die.\".\n",
    "\n",
    "Thus, the challenge lies in learning a policy that enables the agent to maneuver the car effectively through the track, avoiding obstacles, and achieving optimal performance based on the reward structure defined in the environment.\n",
    "\n",
    "With the code block below, you can output the action space, observation space and other environment specifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc1028ed-a409-4e27-84ec-06ec26476917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(5)\n",
      "Observation Space:  Box(0, 255, (96, 96, 3), uint8)\n",
      "Env Spec:  EnvSpec(id='CarRacing-v2', entry_point='gymnasium.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'domain_randomize': True, 'continuous': False}, namespace=None, name='CarRacing', version=2, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CarRacing-v2\", domain_randomize=True, continuous=False)\n",
    "\n",
    "print(\"Action Space:\", env.action_space)\n",
    "\n",
    "# Note, in the output Observation Space Box(0, 255, (96, 96, 3), uint8),\n",
    "# 0 and 255 represent the lower and upper limit of the numbers in that\n",
    "# (96, 96, 3) array.\n",
    "print(\"Observation Space: \", env.observation_space)\n",
    "\n",
    "print(\"Env Spec: \", env.spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1425ce-e9d6-4d59-86d2-a2b5c1c111bd",
   "metadata": {},
   "source": [
    "Next, we will show you how the Gymnasium environment works via an agent that makes action decisions randomly. The agent will make 10 sequential actions. After implementing each action, the environment will return the next observation (or the state), the reward for taking that action, if the environment has terminated or truncated (by satisfying the predefined conditions) due to the latest action and information from the environment about the step, i.e. metrics, debug info. Based on the output, you can see that most of the random actions lead to negative rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acccc8c8-bd1f-4261-9862-c3c6223a7565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken: 0, reward: 7.1463768115942035.\n",
      "Action taken: 4, reward: -0.09999999999999964.\n",
      "Action taken: 1, reward: -0.09999999999999964.\n",
      "Action taken: 0, reward: -0.09999999999999964.\n",
      "Action taken: 4, reward: -0.09999999999999964.\n",
      "Action taken: 1, reward: -0.09999999999999964.\n",
      "Action taken: 4, reward: -0.09999999999999964.\n",
      "Action taken: 4, reward: -0.09999999999999964.\n",
      "Action taken: 2, reward: -0.09999999999999964.\n",
      "Action taken: 0, reward: -0.09999999999999964.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# First, we create our environment called CarRacing-v2.\n",
    "env = gym.make(\"CarRacing-v2\", domain_randomize=True, continuous=False)\n",
    "\n",
    "# Then we reset this environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    # Take a random action from the action space.\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Do this action in the environment and get\n",
    "    # next_state, reward, terminated, truncated and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Action taken: {action}, reward: {reward}.\")\n",
    "\n",
    "    # If the game is terminated (in our case we land, crashed) or truncated (timeout),\n",
    "    # reset the environment.\n",
    "    if terminated or truncated:\n",
    "        # Reset the environment\n",
    "        print(\"Environment is reset\")\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84134c7-daea-4aa0-828b-61bacd068494",
   "metadata": {},
   "source": [
    "### Training a smart agent with stable baselines3\n",
    "\n",
    "It is evident that relying on random actions won't lead to success in this car racing game. Our objective is to train a deep neural network by learning knowledge from the environment, enabling it to make action choices that maximize the cumulative reward.\n",
    "\n",
    "In this specific environment, we will use DQN with convolutional neural network (CNN) to train the smart agent. The DQN is trained by minimizing the temporal difference error between the predicted Q-values and the target Q-values. The CNN architecture is used to process the raw pixel input (game frames or observations) and extract relevant features that are useful for making predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188f78ba-d87c-45d9-8266-ed9081e08fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f578e9-9da5-48d6-bec9-2b9e082b1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# In our experiments, using a linear schedule that gradually decreases the\n",
    "# learning rate achieved better performance than using a fixed learning rate.\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412fbd3",
   "metadata": {},
   "source": [
    "To understand what each metric means in the output of the cell below, you can visit [this page](https://stable-baselines3.readthedocs.io/en/master/common/logger.html). Please be advised that we've condensed the output to save page space. Nonetheless, you should still be able to observe the agent's gradual acquisition of environmental knowledge and its corresponding increase in rewards in the displayed output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06f0fc-30c0-4b68-aae9-36b6ab591bac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./dqn_carrace_tensorboard_lr0001_1e7/DQN_1\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -52.4    |\n",
      "|    exploration_rate | 0.996    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 167      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -52.7    |\n",
      "|    exploration_rate | 0.992    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 167      |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 8000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -53.9    |\n",
      "|    exploration_rate | 0.989    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 167      |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 12000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -53.7    |\n",
      "|    exploration_rate | 0.985    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 167      |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 16000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54.4    |\n",
      "|    exploration_rate | 0.981    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 167      |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 20000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54.6    |\n",
      "|    exploration_rate | 0.977    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 167      |\n",
      "|    time_elapsed     | 143      |\n",
      "|    total_timesteps  | 24000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54.8    |\n",
      "|    exploration_rate | 0.973    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 167      |\n",
      "|    time_elapsed     | 167      |\n",
      "|    total_timesteps  | 28000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55      |\n",
      "|    exploration_rate | 0.97     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 166      |\n",
      "|    time_elapsed     | 192      |\n",
      "|    total_timesteps  | 32000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.6    |\n",
      "|    exploration_rate | 0.966    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 166      |\n",
      "|    time_elapsed     | 215      |\n",
      "|    total_timesteps  | 36000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.9    |\n",
      "|    exploration_rate | 0.962    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 166      |\n",
      "|    time_elapsed     | 240      |\n",
      "|    total_timesteps  | 40000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.6    |\n",
      "|    exploration_rate | 0.958    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 166      |\n",
      "|    time_elapsed     | 263      |\n",
      "|    total_timesteps  | 44000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.5    |\n",
      "|    exploration_rate | 0.954    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 166      |\n",
      "|    time_elapsed     | 287      |\n",
      "|    total_timesteps  | 48000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.5    |\n",
      "|    exploration_rate | 0.951    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 165      |\n",
      "|    time_elapsed     | 313      |\n",
      "|    total_timesteps  | 52000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.95e-05 |\n",
      "|    loss             | 1.64e-05 |\n",
      "|    n_updates        | 499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.5    |\n",
      "|    exploration_rate | 0.947    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 342      |\n",
      "|    total_timesteps  | 56000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.94e-05 |\n",
      "|    loss             | 1.75e-05 |\n",
      "|    n_updates        | 1499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.4    |\n",
      "|    exploration_rate | 0.943    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 161      |\n",
      "|    time_elapsed     | 371      |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.94e-05 |\n",
      "|    loss             | 0.000153 |\n",
      "|    n_updates        | 2499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.5    |\n",
      "|    exploration_rate | 0.939    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 159      |\n",
      "|    time_elapsed     | 400      |\n",
      "|    total_timesteps  | 64000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.94e-05 |\n",
      "|    loss             | 0.000309 |\n",
      "|    n_updates        | 3499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.5    |\n",
      "|    exploration_rate | 0.935    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 158      |\n",
      "|    time_elapsed     | 429      |\n",
      "|    total_timesteps  | 68000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.93e-05 |\n",
      "|    loss             | 5.65e-05 |\n",
      "|    n_updates        | 4499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.7    |\n",
      "|    exploration_rate | 0.932    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 157      |\n",
      "|    time_elapsed     | 458      |\n",
      "|    total_timesteps  | 72000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.93e-05 |\n",
      "|    loss             | 0.000135 |\n",
      "|    n_updates        | 5499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.6    |\n",
      "|    exploration_rate | 0.928    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 156      |\n",
      "|    time_elapsed     | 487      |\n",
      "|    total_timesteps  | 76000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.92e-05 |\n",
      "|    loss             | 0.000181 |\n",
      "|    n_updates        | 6499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.2    |\n",
      "|    exploration_rate | 0.924    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 155      |\n",
      "|    time_elapsed     | 515      |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.92e-05 |\n",
      "|    loss             | 5.45e-05 |\n",
      "|    n_updates        | 7499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.4    |\n",
      "|    exploration_rate | 0.92     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 154      |\n",
      "|    time_elapsed     | 544      |\n",
      "|    total_timesteps  | 84000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.92e-05 |\n",
      "|    loss             | 0.0799   |\n",
      "|    n_updates        | 8499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.2    |\n",
      "|    exploration_rate | 0.916    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 153      |\n",
      "|    time_elapsed     | 572      |\n",
      "|    total_timesteps  | 88000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.91e-05 |\n",
      "|    loss             | 8.21e-05 |\n",
      "|    n_updates        | 9499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.3    |\n",
      "|    exploration_rate | 0.913    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 152      |\n",
      "|    time_elapsed     | 601      |\n",
      "|    total_timesteps  | 92000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.91e-05 |\n",
      "|    loss             | 0.000234 |\n",
      "|    n_updates        | 10499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.5    |\n",
      "|    exploration_rate | 0.909    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 152      |\n",
      "|    time_elapsed     | 630      |\n",
      "|    total_timesteps  | 96000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.9e-05  |\n",
      "|    loss             | 0.000136 |\n",
      "|    n_updates        | 11499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-93.13 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -93.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.9e-05  |\n",
      "|    loss             | 0.000131 |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.5    |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 99       |\n",
      "|    time_elapsed     | 1008     |\n",
      "|    total_timesteps  | 100000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.6    |\n",
      "|    exploration_rate | 0.901    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 100      |\n",
      "|    time_elapsed     | 1037     |\n",
      "|    total_timesteps  | 104000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.9e-05  |\n",
      "|    loss             | 6.27e-05 |\n",
      "|    n_updates        | 13499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.6    |\n",
      "|    exploration_rate | 0.897    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 101      |\n",
      "|    time_elapsed     | 1065     |\n",
      "|    total_timesteps  | 108000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.89e-05 |\n",
      "|    loss             | 0.168    |\n",
      "|    n_updates        | 14499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.7    |\n",
      "|    exploration_rate | 0.894    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 102      |\n",
      "|    time_elapsed     | 1094     |\n",
      "|    total_timesteps  | 112000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.89e-05 |\n",
      "|    loss             | 3.04e-05 |\n",
      "|    n_updates        | 15499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.8    |\n",
      "|    exploration_rate | 0.89     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 103      |\n",
      "|    time_elapsed     | 1123     |\n",
      "|    total_timesteps  | 116000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.88e-05 |\n",
      "|    loss             | 0.00047  |\n",
      "|    n_updates        | 16499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.6    |\n",
      "|    exploration_rate | 0.886    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 104      |\n",
      "|    time_elapsed     | 1151     |\n",
      "|    total_timesteps  | 120000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.88e-05 |\n",
      "|    loss             | 0.099    |\n",
      "|    n_updates        | 17499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.4    |\n",
      "|    exploration_rate | 0.882    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 105      |\n",
      "|    time_elapsed     | 1180     |\n",
      "|    total_timesteps  | 124000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.88e-05 |\n",
      "|    loss             | 0.000127 |\n",
      "|    n_updates        | 18499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.4    |\n",
      "|    exploration_rate | 0.878    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 105      |\n",
      "|    time_elapsed     | 1209     |\n",
      "|    total_timesteps  | 128000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.87e-05 |\n",
      "|    loss             | 0.105    |\n",
      "|    n_updates        | 19499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.5    |\n",
      "|    exploration_rate | 0.875    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 106      |\n",
      "|    time_elapsed     | 1238     |\n",
      "|    total_timesteps  | 132000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.87e-05 |\n",
      "|    loss             | 4.59e-05 |\n",
      "|    n_updates        | 20499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.4    |\n",
      "|    exploration_rate | 0.871    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 107      |\n",
      "|    time_elapsed     | 1267     |\n",
      "|    total_timesteps  | 136000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.86e-05 |\n",
      "|    loss             | 4.38e-05 |\n",
      "|    n_updates        | 21499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.3    |\n",
      "|    exploration_rate | 0.867    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 107      |\n",
      "|    time_elapsed     | 1296     |\n",
      "|    total_timesteps  | 140000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.86e-05 |\n",
      "|    loss             | 3.05e-05 |\n",
      "|    n_updates        | 22499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.5    |\n",
      "|    exploration_rate | 0.863    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 108      |\n",
      "|    time_elapsed     | 1326     |\n",
      "|    total_timesteps  | 144000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.86e-05 |\n",
      "|    loss             | 3.75e-05 |\n",
      "|    n_updates        | 23499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.6    |\n",
      "|    exploration_rate | 0.859    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 109      |\n",
      "|    time_elapsed     | 1355     |\n",
      "|    total_timesteps  | 148000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.85e-05 |\n",
      "|    loss             | 0.000138 |\n",
      "|    n_updates        | 24499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.6    |\n",
      "|    exploration_rate | 0.856    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 109      |\n",
      "|    time_elapsed     | 1384     |\n",
      "|    total_timesteps  | 152000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.85e-05 |\n",
      "|    loss             | 0.0787   |\n",
      "|    n_updates        | 25499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.3    |\n",
      "|    exploration_rate | 0.852    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 110      |\n",
      "|    time_elapsed     | 1413     |\n",
      "|    total_timesteps  | 156000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.84e-05 |\n",
      "|    loss             | 1.29e-05 |\n",
      "|    n_updates        | 26499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55.1    |\n",
      "|    exploration_rate | 0.848    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 110      |\n",
      "|    time_elapsed     | 1442     |\n",
      "|    total_timesteps  | 160000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.84e-05 |\n",
      "|    loss             | 7.61e-05 |\n",
      "|    n_updates        | 27499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55      |\n",
      "|    exploration_rate | 0.844    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 111      |\n",
      "|    time_elapsed     | 1471     |\n",
      "|    total_timesteps  | 164000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.84e-05 |\n",
      "|    loss             | 0.107    |\n",
      "|    n_updates        | 28499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -55      |\n",
      "|    exploration_rate | 0.84     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 111      |\n",
      "|    time_elapsed     | 1500     |\n",
      "|    total_timesteps  | 168000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.83e-05 |\n",
      "|    loss             | 0.183    |\n",
      "|    n_updates        | 29499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54.9    |\n",
      "|    exploration_rate | 0.837    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 112      |\n",
      "|    time_elapsed     | 1529     |\n",
      "|    total_timesteps  | 172000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.83e-05 |\n",
      "|    loss             | 7.12e-05 |\n",
      "|    n_updates        | 30499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54.9    |\n",
      "|    exploration_rate | 0.833    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 112      |\n",
      "|    time_elapsed     | 1558     |\n",
      "|    total_timesteps  | 176000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.82e-05 |\n",
      "|    loss             | 0.103    |\n",
      "|    n_updates        | 31499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54.9    |\n",
      "|    exploration_rate | 0.829    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 113      |\n",
      "|    time_elapsed     | 1586     |\n",
      "|    total_timesteps  | 180000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.82e-05 |\n",
      "|    loss             | 0.00555  |\n",
      "|    n_updates        | 32499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54.7    |\n",
      "|    exploration_rate | 0.825    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 113      |\n",
      "|    time_elapsed     | 1614     |\n",
      "|    total_timesteps  | 184000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.82e-05 |\n",
      "|    loss             | 0.0937   |\n",
      "|    n_updates        | 33499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54.7    |\n",
      "|    exploration_rate | 0.821    |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 114      |\n",
      "|    time_elapsed     | 1643     |\n",
      "|    total_timesteps  | 188000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.81e-05 |\n",
      "|    loss             | 0.000139 |\n",
      "|    n_updates        | 34499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54.5    |\n",
      "|    exploration_rate | 0.818    |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 114      |\n",
      "|    time_elapsed     | 1671     |\n",
      "|    total_timesteps  | 192000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.81e-05 |\n",
      "|    loss             | 0.00454  |\n",
      "|    n_updates        | 35499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54.2    |\n",
      "|    exploration_rate | 0.814    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 115      |\n",
      "|    time_elapsed     | 1699     |\n",
      "|    total_timesteps  | 196000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.8e-05  |\n",
      "|    loss             | 0.105    |\n",
      "|    n_updates        | 36499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-55.62 +/- 28.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -55.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.8e-05  |\n",
      "|    loss             | 0.00438  |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54.3    |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 96       |\n",
      "|    time_elapsed     | 2067     |\n",
      "|    total_timesteps  | 200000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54.1    |\n",
      "|    exploration_rate | 0.806    |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 97       |\n",
      "|    time_elapsed     | 2096     |\n",
      "|    total_timesteps  | 204000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.8e-05  |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 38499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54      |\n",
      "|    exploration_rate | 0.802    |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 97       |\n",
      "|    time_elapsed     | 2125     |\n",
      "|    total_timesteps  | 208000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.79e-05 |\n",
      "|    loss             | 7.37e-06 |\n",
      "|    n_updates        | 39499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -54      |\n",
      "|    exploration_rate | 0.799    |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 98       |\n",
      "|    time_elapsed     | 2155     |\n",
      "|    total_timesteps  | 212000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.79e-05 |\n",
      "|    loss             | 4.52e-05 |\n",
      "|    n_updates        | 40499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -53.6    |\n",
      "|    exploration_rate | 0.795    |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 98       |\n",
      "|    time_elapsed     | 2184     |\n",
      "|    total_timesteps  | 216000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.78e-05 |\n",
      "|    loss             | 0.0854   |\n",
      "|    n_updates        | 41499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -53.9    |\n",
      "|    exploration_rate | 0.791    |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 99       |\n",
      "|    time_elapsed     | 2213     |\n",
      "|    total_timesteps  | 220000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.78e-05 |\n",
      "|    loss             | 0.000724 |\n",
      "|    n_updates        | 42499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -53.9    |\n",
      "|    exploration_rate | 0.787    |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 99       |\n",
      "|    time_elapsed     | 2242     |\n",
      "|    total_timesteps  | 224000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.78e-05 |\n",
      "|    loss             | 0.000166 |\n",
      "|    n_updates        | 43499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -53.7    |\n",
      "|    exploration_rate | 0.783    |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 100      |\n",
      "|    time_elapsed     | 2270     |\n",
      "|    total_timesteps  | 228000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.77e-05 |\n",
      "|    loss             | 1.63e-05 |\n",
      "|    n_updates        | 44499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -53.4    |\n",
      "|    exploration_rate | 0.78     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 100      |\n",
      "|    time_elapsed     | 2300     |\n",
      "|    total_timesteps  | 232000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.77e-05 |\n",
      "|    loss             | 0.000276 |\n",
      "|    n_updates        | 45499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -52.9    |\n",
      "|    exploration_rate | 0.776    |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 101      |\n",
      "|    time_elapsed     | 2329     |\n",
      "|    total_timesteps  | 236000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.76e-05 |\n",
      "|    loss             | 0.0924   |\n",
      "|    n_updates        | 46499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -52.9    |\n",
      "|    exploration_rate | 0.772    |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 101      |\n",
      "|    time_elapsed     | 2359     |\n",
      "|    total_timesteps  | 240000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.76e-05 |\n",
      "|    loss             | 6.19e-05 |\n",
      "|    n_updates        | 47499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -52.5    |\n",
      "|    exploration_rate | 0.768    |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 102      |\n",
      "|    time_elapsed     | 2388     |\n",
      "|    total_timesteps  | 244000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.76e-05 |\n",
      "|    loss             | 0.102    |\n",
      "|    n_updates        | 48499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -52.2    |\n",
      "|    exploration_rate | 0.764    |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 102      |\n",
      "|    time_elapsed     | 2418     |\n",
      "|    total_timesteps  | 248000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.75e-05 |\n",
      "|    loss             | 0.000215 |\n",
      "|    n_updates        | 49499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -51.9    |\n",
      "|    exploration_rate | 0.761    |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 102      |\n",
      "|    time_elapsed     | 2447     |\n",
      "|    total_timesteps  | 252000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.75e-05 |\n",
      "|    loss             | 2.75e-05 |\n",
      "|    n_updates        | 50499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -51.9    |\n",
      "|    exploration_rate | 0.757    |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 103      |\n",
      "|    time_elapsed     | 2477     |\n",
      "|    total_timesteps  | 256000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.74e-05 |\n",
      "|    loss             | 0.000361 |\n",
      "|    n_updates        | 51499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -52.2    |\n",
      "|    exploration_rate | 0.753    |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 103      |\n",
      "|    time_elapsed     | 2507     |\n",
      "|    total_timesteps  | 260000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 9.74e-05 |\n",
      "|    loss             | 0.000375 |\n",
      "|    n_updates        | 52499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 932      |\n",
      "|    ep_rew_mean      | 10.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1320     |\n",
      "|    fps              | 95       |\n",
      "|    time_elapsed     | 13426    |\n",
      "|    total_timesteps  | 1283220  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 8.72e-05 |\n",
      "|    loss             | 0.0743   |\n",
      "|    n_updates        | 308304   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 932      |\n",
      "|    ep_rew_mean      | 10.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1324     |\n",
      "|    fps              | 95       |\n",
      "|    time_elapsed     | 13458    |\n",
      "|    total_timesteps  | 1287220  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 8.71e-05 |\n",
      "|    loss             | 0.132    |\n",
      "|    n_updates        | 309304   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 932      |\n",
      "|    ep_rew_mean      | 10.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1328     |\n",
      "|    fps              | 95       |\n",
      "|    time_elapsed     | 13489    |\n",
      "|    total_timesteps  | 1291220  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 8.71e-05 |\n",
      "|    loss             | 0.0148   |\n",
      "|    n_updates        | 310304   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 925      |\n",
      "|    ep_rew_mean      | 9.06     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1332     |\n",
      "|    fps              | 95       |\n",
      "|    time_elapsed     | 13514    |\n",
      "|    total_timesteps  | 1294487  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 8.71e-05 |\n",
      "|    loss             | 0.209    |\n",
      "|    n_updates        | 311121   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 932      |\n",
      "|    ep_rew_mean      | 8.41     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1336     |\n",
      "|    fps              | 95       |\n",
      "|    time_elapsed     | 13546    |\n",
      "|    total_timesteps  | 1298487  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 8.7e-05  |\n",
      "|    loss             | 0.241    |\n",
      "|    n_updates        | 312121   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=25.60 +/- 62.80\n",
      "Episode length: 873.48 +/- 253.59\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 873      |\n",
      "|    mean_reward      | 25.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1300000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 8.7e-05  |\n",
      "|    loss             | 0.0611   |\n",
      "|    n_updates        | 312499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 926      |\n",
      "|    ep_rew_mean      | 4.18     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1340     |\n",
      "|    fps              | 93       |\n",
      "|    time_elapsed     | 13878    |\n",
      "|    total_timesteps  | 1302000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 8.7e-05  |\n",
      "|    loss             | 0.0935   |\n",
      "|    n_updates        | 312999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 911      |\n",
      "|    ep_rew_mean      | 2.5      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1344     |\n",
      "|    fps              | 93       |\n",
      "|    time_elapsed     | 13898    |\n",
      "|    total_timesteps  | 1304554  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 8.7e-05  |\n",
      "|    loss             | 0.09     |\n",
      "|    n_updates        | 313638   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 911      |\n",
      "|    ep_rew_mean      | 2.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1348     |\n",
      "|    fps              | 93       |\n",
      "|    time_elapsed     | 13929    |\n",
      "|    total_timesteps  | 1308554  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 8.69e-05 |\n",
      "|    loss             | 0.00484  |\n",
      "|    n_updates        | 314638   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 917      |\n",
      "|    ep_rew_mean      | 1.86     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1352     |\n",
      "|    fps              | 94       |\n",
      "|    time_elapsed     | 13960    |\n",
      "|    total_timesteps  | 1312554  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 8.69e-05 |\n",
      "|    loss             | 0.0399   |\n",
      "|    n_updates        | 315638   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "Truncated the output to save space for blog purpose\n",
      "Truncated the output to save space for blog purpose\n",
      "Truncated the output to save space for blog purpose\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 963      |\n",
      "|    ep_rew_mean      | 833      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4000     |\n",
      "|    fps              | 90       |\n",
      "|    time_elapsed     | 42973    |\n",
      "|    total_timesteps  | 3908624  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.09e-05 |\n",
      "|    loss             | 0.837    |\n",
      "|    n_updates        | 964655   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 965      |\n",
      "|    ep_rew_mean      | 830      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4004     |\n",
      "|    fps              | 90       |\n",
      "|    time_elapsed     | 43003    |\n",
      "|    total_timesteps  | 3912482  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.09e-05 |\n",
      "|    loss             | 1.1      |\n",
      "|    n_updates        | 965620   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 965      |\n",
      "|    ep_rew_mean      | 823      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4008     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43034    |\n",
      "|    total_timesteps  | 3916461  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.08e-05 |\n",
      "|    loss             | 0.687    |\n",
      "|    n_updates        | 966615   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 964      |\n",
      "|    ep_rew_mean      | 827      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4012     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43062    |\n",
      "|    total_timesteps  | 3920087  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.08e-05 |\n",
      "|    loss             | 0.891    |\n",
      "|    n_updates        | 967521   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 962      |\n",
      "|    ep_rew_mean      | 833      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4016     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43092    |\n",
      "|    total_timesteps  | 3923873  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.08e-05 |\n",
      "|    loss             | 0.892    |\n",
      "|    n_updates        | 968468   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 962      |\n",
      "|    ep_rew_mean      | 834      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4020     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43124    |\n",
      "|    total_timesteps  | 3927873  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.07e-05 |\n",
      "|    loss             | 0.538    |\n",
      "|    n_updates        | 969468   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 960      |\n",
      "|    ep_rew_mean      | 844      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4024     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43154    |\n",
      "|    total_timesteps  | 3931567  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.07e-05 |\n",
      "|    loss             | 0.579    |\n",
      "|    n_updates        | 970391   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 959      |\n",
      "|    ep_rew_mean      | 844      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4028     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43184    |\n",
      "|    total_timesteps  | 3935404  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.06e-05 |\n",
      "|    loss             | 0.61     |\n",
      "|    n_updates        | 971350   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 963      |\n",
      "|    ep_rew_mean      | 842      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4032     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43215    |\n",
      "|    total_timesteps  | 3939359  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.06e-05 |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 972339   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 961      |\n",
      "|    ep_rew_mean      | 843      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4036     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43243    |\n",
      "|    total_timesteps  | 3943021  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.06e-05 |\n",
      "|    loss             | 1.14     |\n",
      "|    n_updates        | 973255   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 961      |\n",
      "|    ep_rew_mean      | 842      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4040     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43274    |\n",
      "|    total_timesteps  | 3946921  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.05e-05 |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 974230   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 960      |\n",
      "|    ep_rew_mean      | 842      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4044     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43304    |\n",
      "|    total_timesteps  | 3950791  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.05e-05 |\n",
      "|    loss             | 0.446    |\n",
      "|    n_updates        | 975197   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 960      |\n",
      "|    ep_rew_mean      | 843      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4048     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43335    |\n",
      "|    total_timesteps  | 3954685  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.05e-05 |\n",
      "|    loss             | 0.901    |\n",
      "|    n_updates        | 976171   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 960      |\n",
      "|    ep_rew_mean      | 844      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4052     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43365    |\n",
      "|    total_timesteps  | 3958509  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.04e-05 |\n",
      "|    loss             | 0.703    |\n",
      "|    n_updates        | 977127   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 959      |\n",
      "|    ep_rew_mean      | 847      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4056     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43395    |\n",
      "|    total_timesteps  | 3962401  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.04e-05 |\n",
      "|    loss             | 0.687    |\n",
      "|    n_updates        | 978100   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 960      |\n",
      "|    ep_rew_mean      | 844      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4060     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43427    |\n",
      "|    total_timesteps  | 3966401  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.03e-05 |\n",
      "|    loss             | 0.669    |\n",
      "|    n_updates        | 979100   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 960      |\n",
      "|    ep_rew_mean      | 846      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4064     |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 43456    |\n",
      "|    total_timesteps  | 3970095  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.03e-05 |\n",
      "|    loss             | 0.838    |\n",
      "|    n_updates        | 980023   |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "env = gym.make(\"CarRacing-v2\", domain_randomize=True, continuous=False)\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "env.reset(seed=1234)\n",
    "env.action_space.seed(123)\n",
    "\n",
    "# Instantiate the agent and specify the directory for Tensorboard log.\n",
    "# With Tensorboard log, we can monitor the model training progress.\n",
    "model = DQN('CnnPolicy', env, verbose=1, device=\"cuda\", learning_rate=linear_schedule(0.0001), tensorboard_log=\"./dqn_carrace_tensorboard_lr0001_1e7/\")\n",
    "# Specify the directory to save the best model.\n",
    "dnq_path = os.path.join('./Saved_Models/dqn_best_model_lr0001_1e7')\n",
    "\n",
    "# EvalCallback evaluates periodically the performance of an agent, using\n",
    "# a separate test environment. It will save the best model if \n",
    "# best_model_save_path folder is specified.\n",
    "eval_env = model.get_env()\n",
    "eval_callback = EvalCallback(eval_env=eval_env, best_model_save_path=dnq_path,\n",
    "                             n_eval_episodes=50,\n",
    "                             eval_freq=100000,verbose=1,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "# Train the agent.\n",
    "model.learn(total_timesteps=10000000,callback=eval_callback)\n",
    "\n",
    "# Specify the path to save the final model.\n",
    "final_path = os.path.join('./Saved_Models/dqn_final_model_lr0001_1e7')\n",
    "model.save(final_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4201842-21b7-4035-95d7-c9bcaf9935ef",
   "metadata": {},
   "source": [
    "After loading the TensorBoard file (refer to [this tutorial](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks) for guidance), take a look at the reward under eval/mean_reward. You'll notice that the reward converges to around 870 after completing 3.5 million total training timesteps. This is a noteworthy achievement, especially considering that the theoretical maximum reward is below 1000.  Please note that the two lines (dark line and light line) are actually based on the same set of data. Dark line is the smoothed version of the light line (based on raw data) and can better reflect the general trend.\n",
    "\n",
    "<img src='../images/eval_reward.png' />\n",
    "\n",
    "You can find our saved best model `best_model.zip` and TensorBoard file in the `src` folder. With the model file, you can load it and use it to play the game in a way that gives you the highest cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b76c2-47a6-48b1-8876-cdb3e74ab4de",
   "metadata": {},
   "source": [
    "### Visualize the car race\n",
    "\n",
    "Now, let's dive into the exciting part! We can visually compare the performance of the trained agent with that of randomly selecting actions in a cartoon setting. As depicted in the cartoons, when actions are randomly chosen, the car moves slowly and consistently incurs negative rewards (cumulative rewards were shown in the bottom left corner of the cartoon). However, with the trained agent, the car accelerates quickly and adeptly navigates through various road conditions. Pretty amazing, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7af040d3-8a71-4130-a6e0-6ce2dbbe0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt update -y && sudo apt upgrade -y\n",
    "!sudo apt install ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b61bd8e-01eb-404f-80f1-489670bb916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import pyplot as plt, animation\n",
    "from IPython.display import HTML\n",
    "from stable_baselines3 import DQN\n",
    "rcParams['animation.embed_limit'] = 2**128\n",
    "\n",
    "# Define some utility functions for generating and saving the cartoons.\n",
    "def create_anim(frames, dpi, fps):\n",
    "    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    def setup():\n",
    "        plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, init_func=setup, frames=len(frames), interval=fps)\n",
    "    return anim\n",
    "\n",
    "def display_anim(frames, dpi=72, fps=60):\n",
    "    anim = create_anim(frames, dpi, fps)\n",
    "    return anim.to_jshtml()\n",
    "\n",
    "def save_anim(frames, filename, dpi=72, fps=50):\n",
    "    anim = create_anim(frames, dpi, fps)\n",
    "    anim.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e8b4398-aec1-4d7f-979b-3cbbb0626d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-56.20437956204454\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v2\", domain_randomize=True, continuous=False, render_mode='rgb_array')\n",
    "\n",
    "# Generate observations of one episode with random actions.\n",
    "frames = []\n",
    "episodes = 1\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    terminated = truncated = False\n",
    "    score = 0\n",
    "    \n",
    "    while not (terminated or truncated):\n",
    "        frames.append(env.render())\n",
    "        # Note: here we are randomly sampling an action.\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode:{} Score:{}\".format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723a37c-2a5d-47da-b3ca-1feab40d56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the cartoon inline. This agent takes random actions.\n",
    "# Due to the size of the video is too large, we removed the output.\n",
    "# You can refer to the 'src' folder to see the generated video\n",
    "# saved with the step listed below.\n",
    "HTML(display_anim(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8222c02c-2a21-4695-85b0-0c0a7a793b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the cartoon. We provided the generated cartoon in the /src folder.\n",
    "filename = 'CarRacing-v2_random.mp4'\n",
    "save_anim(frames, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ef5054-79e0-449f-82ac-587026d6259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model based on evaluation. You may need to change the model \n",
    "# path depending on where your best model is stored.\n",
    "model_path = \"./Saved_Models/dqn_best_model_lr0001_1e7/best_model\"\n",
    "model = DQN.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6426b3e-5cd6-40a3-8c3d-e1465de00942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:917.1999999999867\n"
     ]
    }
   ],
   "source": [
    "# Generate observations of one episode with actions predicted by the best model.\n",
    "\n",
    "frames = []\n",
    "episodes = 1\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    observation, _ = env.reset()\n",
    "    terminated = truncated = False\n",
    "    score = 0\n",
    "    \n",
    "    while not (terminated or truncated):\n",
    "        frames.append(env.render())\n",
    "        action , _ = model.predict(observation) \n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode:{} Score:{}\".format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27abd7-758d-4927-b518-11cd3c37e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the cartoon inline. This agent takes random actions.\n",
    "# Due to the size of the video is too large, we removed the output.\n",
    "# You can refer to the 'src' folder to see the generated video\n",
    "# saved with the step listed below.\n",
    "HTML(display_anim(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e28c83e-c1fa-4a92-bb1b-3982b687072b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the cartoon. We provided the generated cartoon in the /src folder.\n",
    "filename = 'CarRacing-v2_best.mp4'\n",
    "save_anim(frames, filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1786ce9-abe8-419e-a5c9-c3acaaecd349",
   "metadata": {},
   "source": [
    "### CPU vs GPU performance comparison\n",
    "\n",
    "Given their capacity for parallel processing in the training of deep learning models, GPUs typically outperform CPUs significantly in this domain. In the code block below, we illustrate that AMD GPUs can be several times faster than CPUs. The specific numerical differences may vary depending on the models of hardware you are utilizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "833f2bb6-967f-4df9-9eff-f42efa7d767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with gpu : 652.19s\n",
      "Time with cpu : 5837.96s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from stable_baselines3 import DQN\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CarRacing-v2\", domain_randomize=True,continuous=False)\n",
    "env.reset(seed=1234)\n",
    "env.action_space.seed(123)\n",
    "t1 = time.time()\n",
    "model = DQN(\"CnnPolicy\", env, verbose=0, device=\"cuda\")\n",
    "model.learn(total_timesteps=100000)\n",
    "print(f\"Time with gpu : {time.time()-t1:.2f}s\")\n",
    "\n",
    "env.reset(seed=1234)\n",
    "env.action_space.seed(123)\n",
    "t1 = time.time()\n",
    "model = DQN(\"CnnPolicy\", env, verbose=0, device=\"cpu\")\n",
    "model.learn(total_timesteps=100000)\n",
    "print(f\"Time with cpu : {time.time()-t1:.2f}s\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707bc4d-27ee-4030-ab16-a42fc8a3cbbb",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "\n",
    "\n",
    "We express gratitude to the authors of [this notebook](https://www.kaggle.com/code/manthanbhagat/car-racing-stable-baselines/notebook), whose work serves as a valuable reference for our blog."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
