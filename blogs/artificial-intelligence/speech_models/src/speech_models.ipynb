{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4016dbc6-b30c-49af-b0a1-255cd03b0c3e",
   "metadata": {},
   "source": [
    "# AMD at Work: Fine-tuning and Testing Cutting-Edge Speech Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8426e3-eef2-432c-84f2-99640f0867b8",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b24c2-0063-4b40-9847-42df87485b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, TrainingArguments, Trainer, Wav2Vec2ForCTC, AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "from huggingface_hub import login\n",
    "from unidecode import unidecode\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_dataset, load_metric, DatasetDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "\n",
    "import random\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3ce85-92b6-4061-b3d5-d1772966917c",
   "metadata": {},
   "source": [
    "## Explore the google/fleurs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d0cd9-a759-49ff-b83c-f57c1c27dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"google/fleurs\", \n",
    "    \"es_419\", \n",
    "    split={'train':'train', 'test':'test', 'validation':'validation'},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "'''\n",
    "Google/fleurs dataset contains some inconsistent audio examples depending on the\n",
    "language selected. For the Spanish language, one workaround is to filter for those \n",
    "invalid records by noticing that the maximum value of the waveform is around 1e-4. \n",
    "For more information see the corresponding discussion on Hugging Face:\n",
    "https://huggingface.co/datasets/google/fleurs/discussions/16\n",
    "'''\n",
    "dataset = dataset.filter(lambda example: example['audio']['array'].max()>1e-4)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea79ebc-7bde-4806-bc0e-9dbb3fe20eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the first record on train split\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0ff84-297f-4b8b-8327-7b7bded3849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries with label to id and viceversa\n",
    "labels = dataset[\"train\"].features[\"gender\"].names[:2] # Extract gender of person's speech\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "\n",
    "# Explore some dataset examples\n",
    "idx_list = []\n",
    "num_examples = 5\n",
    "\n",
    "for _ in range(num_examples):\n",
    "    rand_idx = random.randint(0, len(dataset[\"train\"])-1)\n",
    "    example = dataset[\"train\"][rand_idx] # select a random example\n",
    "    audio = example[\"audio\"] # extract waveform\n",
    "    idx_list.append(rand_idx) \n",
    "\n",
    "    print(f'Item: {rand_idx} | Label: {id2label[str(example[\"gender\"])]}={label2id[id2label[str(example[\"gender\"])]]}')\n",
    "    print(f'Shape: {audio[\"array\"].shape}, sampling rate: {audio[\"sampling_rate\"]}')\n",
    "    display(Audio(audio[\"array\"], rate=audio[\"sampling_rate\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9585ae-7a86-4b5e-84be-5f1ee619ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the corresponding Raw text transcription of each audio record\n",
    "pd.DataFrame({'sentence':dataset['train'][idx_list]['raw_transcription']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7db0f3-9955-49be-9949-ad1760108f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogtram of duration of audio records in train split\n",
    "\n",
    "sampling_rate = 16000\n",
    "\n",
    "duration_in_seconds = pd.Series([len(k['audio']['array'])/sampling_rate for k in dataset['train']])\n",
    "\n",
    "ax = duration_in_seconds.hist(rwidth = 0.8)\n",
    "ax.set_xlabel('Duration in seconds')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.grid(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_title('Histogram of speech duration | Train split')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290e774-9502-4c4c-a9b8-aef75e99b6de",
   "metadata": {},
   "source": [
    "## Automatic Speech Recognition in Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca81ec-9ad9-4c29-adca-712417bf392e",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96efc05-91e1-4166-b549-ab8a34077307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "\n",
    "    def __init__(self, processor, padding = True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "\n",
    "        # Split input and labels. They might need different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad input features\n",
    "        batch = self.processor.pad(input_features, padding = self.padding, return_tensors = \"pt\")\n",
    "\n",
    "        # Prepare labels for processing and use processor\n",
    "        label_texts = [self.processor.decode(feature[\"input_ids\"], skip_special_tokens = True) for feature in label_features]\n",
    "        labels_batch = self.processor(text = label_texts, padding = self.padding, return_tensors = \"pt\")\n",
    "        \n",
    "        # Replace padding with -100 to ignore\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1),-100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b2c55-4a73-430a-9786-10d70c2df5d7",
   "metadata": {},
   "source": [
    "### Finetuning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c2d32-4214-4034-b774-858e5952eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRFineTuner:\n",
    "\n",
    "    def __init__(self, pretrained_model_tag, dataset_name, output_dir, num_train_epochs = 5, learning_rate=3e-4, batch_size = 16):\n",
    "        \n",
    "        self.pretrained_model_tag = pretrained_model_tag\n",
    "        self.dataset_name = dataset_name\n",
    "        self.output_dir = output_dir\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Load and preprocess dataset\n",
    "        self.dataset = load_dataset(self.dataset_name, \"es_419\", split={'train':'train', 'test':'test', 'validation':'validation'}, trust_remote_code=True)\n",
    "        self.dataset = self.dataset.filter(lambda example: example['audio']['array'].max()>1e-4) #remove invalid examples\n",
    "        \n",
    "        self.tokenized_dataset =  self.dataset.map(self._remove_special_characters)\n",
    "        self._create_vocabulary_json() # Create vocabulary tokens file\n",
    "        \n",
    "        self.vocab_dict = None # contains the vocabulary letters. For display only\n",
    "\n",
    "        # Load tokenizer, feature extractor, processor\n",
    "        self.tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\",)\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "        self.processor = Wav2Vec2Processor(feature_extractor = self.feature_extractor, tokenizer = self.tokenizer)\n",
    "\n",
    "        # Tokenize dataset\n",
    "        self.tokenized_dataset = self.tokenized_dataset.map(self._prepare_dataset, num_proc=4, remove_columns=self.dataset.column_names[\"train\"]) \n",
    "        self.train_dataset = self.tokenized_dataset['train']\n",
    "        self.test_dataset = self.tokenized_dataset['test']\n",
    "        self.validation_dataset = self.tokenized_dataset['validation']        \n",
    "\n",
    "        # Instantiate data collator\n",
    "        self.data_collator = DataCollatorCTCWithPadding(processor=self.processor, padding=True)\n",
    "\n",
    "        # Load the model\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(\n",
    "            self.pretrained_model_tag, \n",
    "            attention_dropout=0.1,\n",
    "            hidden_dropout=0.1,\n",
    "            feat_proj_dropout=0.0,\n",
    "            mask_time_prob=0.05,\n",
    "            layerdrop=0.1,\n",
    "            ctc_loss_reduction=\"mean\", \n",
    "            pad_token_id = self.processor.tokenizer.pad_token_id,\n",
    "            vocab_size = len(self.processor.tokenizer)\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Wav2Vec2 layers are used to extract acoustic features from the raw speech signal. \n",
    "        # thus the feaure extraction part of the model has been sufficiently trained and does not need additional fine-tune\n",
    "        self.model.freeze_feature_encoder() \n",
    "\n",
    "        # Gradient checkpointing reduces memory footprint during training  by storing\n",
    "        # only a subset of intermediate activations and recomputing the rest during backward pass\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        \n",
    "        \n",
    "        # Training arguments\n",
    "        self.training_args = TrainingArguments(\n",
    "            output_dir = self.output_dir,\n",
    "            group_by_length = True,\n",
    "            per_device_train_batch_size = 4,\n",
    "            per_device_eval_batch_size= 4,\n",
    "            eval_strategy = \"epoch\",\n",
    "            num_train_epochs=self.num_train_epochs,\n",
    "            fp16=True, #enabled mixed precision\n",
    "            save_steps = 100,\n",
    "            eval_steps = 100,\n",
    "            logging_steps = 10,\n",
    "            learning_rate = self.learning_rate,\n",
    "            warmup_steps = 50,\n",
    "            save_total_limit = 2,\n",
    "            push_to_hub = False\n",
    "        )\n",
    "\n",
    "    \n",
    "        # Trainer\n",
    "        self.trainer = Trainer(\n",
    "            model = self.model,\n",
    "            data_collator = self.data_collator,\n",
    "            args = self.training_args,\n",
    "            compute_metrics = self._compute_metrics,\n",
    "            train_dataset = self.train_dataset,\n",
    "            eval_dataset = self.validation_dataset,\n",
    "            tokenizer = self.processor.feature_extractor,\n",
    "        )\n",
    "\n",
    "        # Inference results\n",
    "        self.results = None\n",
    "        \n",
    "\n",
    "    # -- Helper methods --\n",
    "\n",
    "    def _prepare_dataset(self, batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        \n",
    "        # batched input_values and labels\n",
    "        batch[\"input_values\"] = self.processor(audio[\"array\"], sampling_rate=16000).input_values[0]\n",
    "        batch[\"labels\"] = self.processor(text = batch['raw_transcription']).input_ids\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    def _remove_special_characters(self,batch):\n",
    "        chars_to_ignore_regex =  \"[.,?!;:'-=@$#<>\\[\\]_{}|&`~'*\\/()+%0-9']\"\n",
    "        batch[\"raw_transcription\"] = re.sub(chars_to_ignore_regex, \"\",unidecode(batch[\"raw_transcription\"])).lower() + \" \"\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    def _extract_all_chars(self,batch):\n",
    "      all_text = \" \".join(batch[\"raw_transcription\"])\n",
    "      vocab = list(set(all_text))\n",
    "        \n",
    "      return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "    def _create_vocabulary_json(self):\n",
    "        # Aggreagates all the transcription text\n",
    "        vocabs = self.tokenized_dataset.map(\n",
    "            self._extract_all_chars, \n",
    "            batched=True, \n",
    "            batch_size=-1,\n",
    "            keep_in_memory=True,\n",
    "            remove_columns=self.dataset.column_names[\"train\"]\n",
    "        )\n",
    "\n",
    "        # Create a vocabulary (letters) dictionary\n",
    "        vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]) | set(vocabs[\"validation\"][\"vocab\"][0]))\n",
    "        vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "        vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "        del vocab_dict[\" \"]\n",
    "        vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "        vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "        # Save the vocabulary as json for Wav2Vec2CTCTokenizer\n",
    "        with open('vocab.json', 'w') as vocab_file:\n",
    "            json.dump(vocab_dict, vocab_file)\n",
    "\n",
    "        self.vocab_dict = vocab_dict\n",
    "\n",
    "    def _compute_metrics(self, pred):\n",
    "        pred_logits = pred.predictions\n",
    "        pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    \n",
    "        pred.label_ids[pred.label_ids == -100] = self.processor.tokenizer.pad_token_id\n",
    "    \n",
    "        pred_str = self.processor.batch_decode(pred_ids) #predicted string\n",
    "        label_str = self.processor.batch_decode(pred.label_ids, group_tokens=False) \n",
    "\n",
    "        wer_metric = evaluate.load(\"wer\", trust_remote_code=True) #Word Error Rate metric\n",
    "        wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "        \n",
    "        return {\"wer\": wer}\n",
    "\n",
    "    def _map_to_result(self,batch):        \n",
    "        with torch.no_grad():\n",
    "            input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "            logits = self.model(input_values).logits\n",
    "        \n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        batch[\"pred_str\"] = self.processor.batch_decode(pred_ids)[0]\n",
    "        batch[\"text\"] = self.processor.decode(batch[\"labels\"], group_tokens=False)        \n",
    "        \n",
    "        return batch\n",
    "\n",
    "\n",
    "    # -- Class methods --\n",
    "    def train(self):\n",
    "        self.trainer.train()\n",
    "\n",
    "    def predict_test_set(self):\n",
    "        results = self.test_dataset.map(self._map_to_result, remove_columns = self.test_dataset.column_names)\n",
    "        \n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0178fb-7e0a-4e9f-8d85-96a45bacea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_ASR = ASRFineTuner(\n",
    "    pretrained_model_tag = \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    dataset_name = \"google/fleurs\",\n",
    "    output_dir = './spanish_asr_out',\n",
    "    num_train_epochs = 5\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "spanish_ASR.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07322f61-14b6-4af8-b5be-e4906c3d1251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference \n",
    "results = spanish_ASR.predict_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a273fc7d-726b-46f3-997a-55f2d6b0cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b159ce-f374-4c9c-86d8-a8befb33db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=50):\n",
    "\n",
    "    # Shows 50 examples    \n",
    "    assert num_examples <= len(dataset), \"Not enough elements in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "\n",
    "show_random_elements(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7799f43-29d8-4f60-a001-0a7577fdc647",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b74e1-e54d-4fb6-a71c-be0d6011f0b0",
   "metadata": {},
   "source": [
    "## Audio Spectrogram Transformer for audio classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af5041-f947-479c-8e6f-552bbe864e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTFeatureExtractor\n",
    "from datasets import load_dataset, Audio, DatasetDict, Dataset\n",
    "from transformers import AutoModelForAudioClassification\n",
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb39523-c19d-4f7d-90d7-bd23e6fb5eb1",
   "metadata": {},
   "source": [
    "### Prepare and explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba7ba96-c498-4d46-82b5-35e895b715cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset by selecting a few examples\n",
    "\n",
    "audio_dataset = load_dataset(\"agkphysics/AudioSet\",\n",
    "                             trust_remote_code=True,\n",
    "                             split = \"test\",\n",
    "                             streaming = True\n",
    "                            )\n",
    "\n",
    "audio_dataset_sample = [next(iter(audio_dataset)) for _ in range(50)] # select 50 examples\n",
    "audio_dataset_sample = Dataset.from_list(random.sample(audio_dataset_sample,5)) # dataset with 5 random examples from the 50 before\n",
    "audio_dataset_sample = DatasetDict({'test':audio_dataset_sample}) # transform to datasetdict object\n",
    "audio_dataset_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a599eea-b787-43bf-a269-2ee3e896a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the first example\n",
    "audio_dataset_sample['test']['audio'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb95e18-5a1f-4d4c-8c10-e93b80f01b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling waveform to 16kHz\n",
    "sampling_rate = 16000\n",
    "audio_dataset_sample = audio_dataset_sample.cast_column('audio', Audio(sampling_rate = sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd18bd4-8aa7-4d79-a160-bf622211eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore audio samples\n",
    "num_examples = 5\n",
    "for k in range(num_examples):\n",
    "    example = audio_dataset_sample['test'][k]\n",
    "    actual_label = example['human_labels']\n",
    "    print(f'True labels: {actual_label}')\n",
    "    display(IPython.display.Audio(data = np.asarray(example['audio']['array']),rate = sampling_rate, autoplay=False) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e26ac-a8d0-4dee-94d5-1060ae9c3764",
   "metadata": {},
   "source": [
    "### Inference: Audio classification on examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010e0bb-24bb-4a62-b154-fc8482485805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate waveforms in a single list\n",
    "waveforms  = [np.asarray(k['audio']['array']) for k in audio_dataset_sample['test']] \n",
    "\n",
    "# Apply feature extractor on waveforms\n",
    "feature_extractor = ASTFeatureExtractor()\n",
    "inputs = feature_extractor(waveforms, sampling_rate=sampling_rate, padding=\"max_length\", return_tensors=\"pt\")\n",
    "input_values = inputs.input_values\n",
    "\n",
    "# Instantiate the model for inference\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "# Set to inference mode\n",
    "with torch.no_grad():\n",
    "  outputs = model(input_values)\n",
    "\n",
    "# Predicted labels\n",
    "predicted_class_ids = outputs.logits.argmax(-1)\n",
    "\n",
    "for id in predicted_class_ids:\n",
    "    print(\"Predicted class:\", model.config.id2label[id.item()])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a672e38-eb88-47dc-9fbb-bcdd56665899",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f1b59-c719-4fd6-9e70-03df557fb289",
   "metadata": {},
   "source": [
    "## Pyannote audio diarization on telephone calls in Spanish language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3b000-97fa-4cd2-afd8-8ca4aa931bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTFeatureExtractor\n",
    "from datasets import load_dataset, Audio, DatasetDict, Dataset\n",
    "from transformers import AutoModelForAudioClassification\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24be73e-f496-4f0d-aa76-575c3b85e377",
   "metadata": {},
   "source": [
    "### Prepare and explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69710579-6696-4f6e-b710-54604cd73992",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"Your_Hugging_Face_Token\"\n",
    "\n",
    "audio_dataset = load_dataset(\"talkbank/callhome\", \n",
    "                             \"spa\", \n",
    "                             trust_remote_code=True, \n",
    "                             split = \"data\", \n",
    "                             streaming = True, \n",
    "                             token= hf_token\n",
    "                            )\n",
    "\n",
    "data_iter = iter(audio_dataset)\n",
    "audio_dataset_sample = [next(data_iter) for _ in range(30)]\n",
    "audio_dataset_sample = Dataset.from_list(random.sample(audio_dataset_sample,3))\n",
    "audio_dataset_sample = DatasetDict({'test':audio_dataset_sample})\n",
    "audio_dataset_sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afefa9-2df7-4984-9bf9-82d5d1dab3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the first example\n",
    "audio_dataset_sample['test']['audio'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78c386-5308-44b6-aea3-878ca1c0e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of Telephone comversations by limiting to 15 seconds of audio\n",
    "\n",
    "secs = 15\n",
    "sampling_rate = 16000\n",
    "num_examples = audio_dataset_sample['test'].num_rows\n",
    "\n",
    "for k in range(num_examples):\n",
    "    example = audio_dataset_sample['test'][k]\n",
    "    \n",
    "    print(f'Telephone conversations: {k+1} of {num_examples}')\n",
    "    conversation_snippet = np.asarray(example['audio']['array'][-secs*sampling_rate:]) #select last 15 seconds of audio\n",
    "    display(IPython.display.Audio(data = conversation_snippet,rate = sampling_rate, autoplay=False) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa355d-a538-4308-8d11-caf35b597eea",
   "metadata": {},
   "source": [
    "### Inference: Audio diarization on first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23bd9f5-becc-4458-8682-271e812c6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"Your_Hugging_Face_Token\"\n",
    "\n",
    "# Load the model\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token = hf_token)\n",
    "pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "# Perform inference on the first Telephone conversation audio example\n",
    "example = audio_dataset_sample['test'][0]\n",
    "waveform_snippet = example['audio']['array'][-secs*sampling_rate:] #slice for the last 15 seconds\n",
    "waveform_snippet = torch.tensor(waveform_snippet, device = 'cuda').unsqueeze(0)\n",
    "\n",
    "# Apply pretrained pipeline\n",
    "diarization = pipeline({\"waveform\":waveform_snippet, \"sample_rate\":sampling_rate})\n",
    "\n",
    "# Print the result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944ed54b-86a0-415c-bbe8-b2484185af3b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cfc4c2-49a8-4b44-a3e1-8dcb23a1ab81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
