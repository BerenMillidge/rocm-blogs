{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b938ca-37e6-47f8-b1df-7274d3a5e0e9",
   "metadata": {},
   "source": [
    "## Explore the dataset for summarization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd9bf5c-e7ff-4119-8dda-e32f7ae04e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b248d8f-9407-44e4-9ff8-4b3d970e75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_dataset = datasets.load_dataset('EdinburghNLP/xsum', trust_remote_code=True, split=\"train[:10%]\")\n",
    "summarization_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5276aa0c-010a-4b7e-bb3f-79daa7d620d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDocument:\\n{summarization_dataset['document'][0]}\")\n",
    "print(f\"\\nSummary:\\n{summarization_dataset['summary'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5798197-8e00-4250-b9e4-dc31eb95378e",
   "metadata": {},
   "source": [
    "## torchtune built-in recipes and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51371136-35cc-4c71-8d22-9ecda80412b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tune ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6220c-22e4-4c10-9da9-5e5b0693be39",
   "metadata": {},
   "source": [
    "## Finetune Llama 3 for summarization tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739d8a8-537d-4944-b3db-3b6a9e344d3e",
   "metadata": {},
   "source": [
    "### Downloading LLaMa3.1-8B-Instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd7bda-2a42-4a14-95e5-447705cb61f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir /tmp/Meta-Llama-3.1-8B-Instruct --ignore-patterns \"original/consolidated.00.pth\" --hf-token YOUR_HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e48e21-c3d5-437a-afa6-b23ca7d86bfa",
   "metadata": {},
   "source": [
    "### Torchtune fine-tuning by copy modifying an existing config file using `tune cp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b96de6e6-61d2-45c3-bdab-275937371ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tune cp llama3_1/8B_lora my_llama3_1_custom_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c3c26-e990-45dc-b66b-831738e02d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated the whole dataset argument with:\n",
    "\n",
    "\"\"\"\n",
    "dataset:\n",
    "  _component_: torchtune.datasets.instruct_dataset\n",
    "  column_map:\n",
    "    dialogue: document\n",
    "    output: summary\n",
    "  max_seq_len: 3072\n",
    "  packed: false\n",
    "  source: EdinburghNLP/xsum\n",
    "  split: train[:1000]\n",
    "  template: torchtune.data.SummarizeTemplate\n",
    "  train_on_input: false\n",
    "  trust_remote_code: true\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2470cf-35b4-47ad-8e64-e4feea6d83e3",
   "metadata": {},
   "source": [
    "### Finetune llama3.1-8B for summarization tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae250e51-899d-433a-bdfc-03f81a1f4eaa",
   "metadata": {},
   "source": [
    "#### a) Finetuning using the configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1adb34-e754-4b7b-963b-1bf2400180c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "! tune run --nproc_per_node 8 lora_finetune_distributed --config my_llama3_1_custom_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab9b1a-1342-4f56-a829-3e8eda68ec46",
   "metadata": {},
   "source": [
    "#### b) Finetuning using the command-line overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b2f1e-e380-440e-967c-e963d90f07b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "! tune run --nproc_per_node 8 lora_finetune_distributed --config llama3_1/8B_lora \\\n",
    "dataset=torchtune.datasets.instruct_dataset \\\n",
    "dataset.source=EdinburghNLP/xsum \\\n",
    "dataset.split=train[:2000] \\\n",
    "dataset.max_seq_len=2048 \\\n",
    "dataset.template=torchtune.data.SummarizeTemplate \\\n",
    "dataset.column_map.dialogue=document \\\n",
    "dataset.column_map.output=summary \\\n",
    "dataset.trust_remote_code=True \\\n",
    "dataset.packed=False \\\n",
    "dataset.train_on_input=False \\\n",
    "epochs=10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60352cf9-4f2c-41a5-8fc9-025c957fa98a",
   "metadata": {},
   "source": [
    "### Testing the fine-tuned model with `tune run generate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72619923-fc9c-42e5-8710-0e12edf4569a",
   "metadata": {},
   "source": [
    "#### a) Making a copy of the generation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23fe021-3893-45e0-a59e-3380ca9ab70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tune cp generation ./my_llama3_1_custom_generation_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e72c6-93b9-4ee0-ae70-6887c4765885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update checkpoint argument with the following\n",
    "\n",
    "\"\"\"\n",
    "checkpointer:\n",
    "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
    "  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/\n",
    "  checkpoint_files: [\n",
    "    hf_model_0001_9.pt,\n",
    "    hf_model_0002_9.pt,\n",
    "    hf_model_0003_9.pt,\n",
    "    hf_model_0004_9.pt,\n",
    "  ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ae8ab-e17e-4fac-ae16-bbc2fffd0707",
   "metadata": {},
   "source": [
    "#### b) Run for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e4f17-d854-48c2-97ad-21e7bc9c5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tune run generate --config ./my_llama3_1_custom_generation_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8157b95-2722-4a01-9e66-1e393c736d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cfed774-0d19-41fe-a509-9b7bc0d7d1b9",
   "metadata": {},
   "source": [
    "## Evaluating scalability on multiples GPUs with Torchtune's distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f382bc9-2ca3-4bc1-8471-9f8826ceaa22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "! tune run --nproc_per_node 2 lora_finetune_distributed --config llama3_1/8B_lora \\\n",
    "dataset=torchtune.datasets.instruct_dataset \\\n",
    "dataset.source=EdinburghNLP/xsum \\\n",
    "dataset.split=train[:2000] \\\n",
    "dataset.max_seq_len=2048 \\\n",
    "dataset.template=torchtune.data.SummarizeTemplate \\\n",
    "dataset.column_map.dialogue=document \\\n",
    "dataset.column_map.output=summary \\\n",
    "dataset.trust_remote_code=True \\\n",
    "dataset.packed=False \\\n",
    "dataset.train_on_input=False \\\n",
    "epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6217b1-e51a-4c6b-8720-944b65cc9719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b03b2663-d619-4f52-b8c6-ec479372fd89",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df2f43-053c-441e-a53b-a764b9bac9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Sample data\n",
    "x_values = [2, 4, 6, 8]\n",
    "y_values = [1216, 792, 662, 527]  # Replace with actual runtime values\n",
    "\n",
    "# Create the bar chart\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(x=x_values, y=y_values)\n",
    "])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Runtime for Fine-Tuning Task',\n",
    "    xaxis_title='Number of GPUs',\n",
    "    yaxis_title='Runtime (seconds)',\n",
    "    template='plotly_white',  # Using the minimal template\n",
    "    width=600,\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb54cb-20a9-43b1-adca-46e7d3624c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
